{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_state, n_action):\n",
    "        super(DQN, self).__init__()        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_state, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_action)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # Get an epsilon greedy action for given state\n",
    "        if random.random() > epsilon: # Use argmax_a Q(s,a)\n",
    "            state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            q_value = q_value.cpu()\n",
    "            action = q_value.max(1)[1].item()            \n",
    "        else: # get random action\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Add batch index dimension to state representations\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import random\n",
    "#from dqn.replay_memory import Transition\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_episode(policy_net, env, memory, device, epsilon = 0.05):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    store = False\n",
    "    for t in count():\n",
    "        if env.to_play() == 1:\n",
    "            q_values = policy_net(state)\n",
    "            rand = random.uniform(0,1)\n",
    "\n",
    "            if rand <= epsilon:\n",
    "                action = random.choice(env.legal_actions())\n",
    "            else:\n",
    "                actions = torch.argsort(q_values,descending=True)\n",
    "                #action = actions[0,0].item()\n",
    "                for a in actions[0,:]:\n",
    "                     if a.item() in env.legal_actions():\n",
    "                        action = a\n",
    "                        break\n",
    "\n",
    "            action_tensor = torch.tensor([[action]], dtype=torch.int64).to(device)\n",
    "            next_state_0, reward, done = env.step(action)\n",
    "            next_state_0 = torch.FloatTensor(next_state_0).unsqueeze(0).to(device)\n",
    "            reward = torch.FloatTensor([reward]).to(device)\n",
    "\n",
    "            next_state_1 = None\n",
    "            store = False\n",
    "        else:\n",
    "            action_exp = env.expert_action()\n",
    "\n",
    "            next_state_1, reward, done = env.step(action_exp)\n",
    "            next_state_1 = torch.FloatTensor(next_state_1).unsqueeze(0).to(device)\n",
    "            reward = torch.FloatTensor([-reward]).to(device)\n",
    "            store = True\n",
    "\n",
    "        if done == True:\n",
    "            memory.push(state, action_tensor, None, reward)\n",
    "            break\n",
    "        else:\n",
    "            if next_state_1 != None:\n",
    "                memory.push(state, action_tensor, next_state_1, reward)\n",
    "        if store == True:\n",
    "            state = next_state_1\n",
    "    return reward.item()\n",
    "\n",
    "def train_network(policy_net, target_net, optimizer, gamma, memory, batch_size):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = memory.sample(batch_size)\n",
    "\n",
    "    batch = Transition(*zip(*batch))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "    batch.next_state)), device=device)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "    if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    q_values = policy_net(state_batch)\n",
    "    state_action_values = q_values.gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "gamma = 0.3\n",
    "memory = ReplayMemory(100000)\n",
    "batch_size = 64\n",
    "target_update = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr = 0.001)\r\n",
    "step_size = 10000\r\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20000\n",
    "losses = []\n",
    "rewards = []\n",
    "reward_sum = 0\n",
    "loss_sum = 0\n",
    "max_epsilon = 0.20\n",
    "min_epsilon = 0.0\n",
    "\n",
    "for epoch in range(1,EPOCH+1):\n",
    "    epsilon = max_epsilon*(1-(epoch/EPOCH)) + min_epsilon*((epoch/EPOCH))\n",
    "    reward = run_episode(policy_net, env, memory, device, epsilon = epsilon)\n",
    "    loss = train_network(policy_net, target_net, optimizer, gamma, memory, ba\n",
    "    if loss == None:\n",
    "        continue\n",
    "    else:\n",
    "        #scheduler.step()\n",
    "        pass\n",
    "    reward_sum += reward\n",
    "    loss_sum += loss\n",
    "    if epoch % 100 == 0:\n",
    "        avg_loss = loss_sum / 100\n",
    "        avg_reward = reward_sum / 100\n",
    "        print(f\"[EPOCH: {epoch}] [LOSS: {avg_loss}] [REWARD: {avg_reward}]\")\n",
    "        losses.append(avg_loss)\n",
    "        rewards.append(avg_reward)\n",
    "        loss_sum = 0\n",
    "        reward_sum = 0\n",
    "    if epoch % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  }
 ]
}