{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fatty-instrumentation",
   "metadata": {},
   "source": [
    "### TASK2: Explore methods for batching patterns of different length prior to presentation to a RNN and implement them. See how much speedup you can get from the GPU with minibatch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hydraulic-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acting-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured device:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "from chosen_gpu import get_freer_gpu\n",
    "device = torch.device(get_freer_gpu()) \n",
    "print(\"Configured device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-springer",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "congressional-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "df = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape') \n",
    "df = df.fillna(method=\"ffill\")\n",
    "df = df.drop(['Tag'], axis =1)\n",
    "df = df.drop(['Sentence #'], axis =1)\n",
    "# words = df['Word']\n",
    "# tags = df['POS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-alfred",
   "metadata": {},
   "source": [
    "#### 1.2 Build the cat_words dictionary, a list of tags per word and a list of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "monetary-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_words = {}\n",
    "all_tags = []\n",
    "all_words = []\n",
    "tags = list(set(df[\"POS\"].values))\n",
    "\n",
    "for i in tags:\n",
    "    cat_words[i] = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    cat_words[df['POS'].iloc[i]].append(df['Word'].iloc[i])\n",
    "    all_tags.append(df['POS'].iloc[i])\n",
    "    all_words.append(df['Word'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "human-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP:131426\n",
      "``:3728\n",
      "VBN:32328\n",
      "PDT:147\n",
      "RB:20252\n",
      "WDT:3698\n",
      "MD:6973\n",
      "CC:23716\n",
      "RBS:296\n",
      "PRP:13318\n",
      "EX:663\n",
      "VBG:19125\n",
      "NN:145807\n",
      "RBR:1055\n",
      "UH:24\n",
      "JJR:2967\n",
      "VBD:39379\n",
      ".:47831\n",
      "TO:23061\n",
      "::795\n",
      "NNPS:2521\n",
      "WP:2542\n",
      ",:32757\n",
      "JJS:3034\n",
      "WRB:2184\n",
      "IN:120996\n",
      "VBP:16158\n",
      "WP$:99\n",
      "DT:98454\n",
      ";:214\n",
      "VBZ:24960\n",
      "FW:1\n",
      "CD:24695\n",
      "LRB:678\n",
      "RRB:679\n",
      "PRP$:8655\n",
      "POS:11257\n",
      "JJ:78412\n",
      "$:1149\n",
      "VB:24211\n",
      "RP:2490\n",
      "NNS:75840\n",
      "1048575\n",
      "1048575\n"
     ]
    }
   ],
   "source": [
    "for i in tags:\n",
    "    print(f'{i}:{len(cat_words[i])}')\n",
    "    \n",
    "print(len(all_words))\n",
    "print(len(all_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-surfing",
   "metadata": {},
   "source": [
    "#### All words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stretch-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!' '\"' '#' ... '\\x96' '\\x97' 'Â°C']\n",
      "35178\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(df['Word']))\n",
    "print(len(np.unique(df['Word'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-favor",
   "metadata": {},
   "source": [
    "#### All tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rocky-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$' ',' '.' ':' ';' 'CC' 'CD' 'DT' 'EX' 'FW' 'IN' 'JJ' 'JJR' 'JJS' 'LRB'\n",
      " 'MD' 'NN' 'NNP' 'NNPS' 'NNS' 'PDT' 'POS' 'PRP' 'PRP$' 'RB' 'RBR' 'RBS'\n",
      " 'RP' 'RRB' 'TO' 'UH' 'VB' 'VBD' 'VBG' 'VBN' 'VBP' 'VBZ' 'WDT' 'WP' 'WP$'\n",
      " 'WRB' '``']\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(tags))\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-rogers",
   "metadata": {},
   "source": [
    "### 2. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "common-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_words, all_tags, test_size = 0.2, random_state = 123) #, stratify = pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "norman-galaxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations in the training data:  838860\n",
      "The number of observations in the test data:  209715\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of observations in the training data: \", len(X_train))\n",
    "print(\"The number of observations in the test data: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-gossip",
   "metadata": {},
   "source": [
    "### 3. Encode words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "owned-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create representation of the name\n",
    "def word_rep(word):\n",
    "    rep = torch.zeros(len(word), 1, n_letters) #Create a zeros tensor\n",
    "    #iterate through all the characters in the name\n",
    "    for index, letter in enumerate(word):\n",
    "        pos = all_letters.find(letter)\n",
    "        rep[index][0][pos] = 1 #Assign a value for each pos value\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "considerable-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create vec representation of the language\n",
    "def tag_rep(tag):\n",
    "    return torch.tensor([tags.index(tag)], dtype = torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-battlefield",
   "metadata": {},
   "source": [
    "#### Example of word and tag representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "stuffed-calvin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([4, 1, 57])\n",
      "tensor([28])\n"
     ]
    }
   ],
   "source": [
    "#example of name representation\n",
    "beau = word_rep(\"beau\")\n",
    "print(beau)\n",
    "print(beau.shape)\n",
    "\n",
    "tag = tag_rep(\"DT\")\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-average",
   "metadata": {},
   "source": [
    "### 4. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "handy-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create simple rnn network \n",
    "import torch.nn as nn\n",
    "class RNN_net(nn.Module):\n",
    "    #Create a constructor\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_net, self).__init__()\n",
    "        self.hidden_size = hidden_size \n",
    "        self.rnn_cell = nn.RNN(input_size, hidden_size)\n",
    "        self.h20 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    #create a forward pass function\n",
    "    def forward(self, input_, hidden = None, batch_size = 1):\n",
    "        out, hidden = self.rnn_cell(input_, hidden)\n",
    "        output = self.h20(hidden.view(-1, self.hidden_size))\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size = 1):\n",
    "        #function to init the hidden layers\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-direction",
   "metadata": {},
   "source": [
    "### 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "continuous-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run interference\n",
    "def infer(net, name, device = \"cpu\"):\n",
    "    name_ohe = word_rep(name).to(device)\n",
    "\n",
    "    #get the output\n",
    "    output, hidden = net(name_ohe)\n",
    "\n",
    "    if type(hidden) is tuple: #for lSTM\n",
    "        hidden = hidden[0]\n",
    "    index = torch.argmax(hidden)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "understanding-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tags present:  42\n"
     ]
    }
   ],
   "source": [
    "#create hidden layers\n",
    "n_hidden = 128 #hidden layers count\n",
    "\n",
    "#number of tags\n",
    "n_tags= len(cat_words.keys())\n",
    "print(\"Total number of tags present: \", n_tags)\n",
    "\n",
    "#initialize the network\n",
    "net = RNN_net(input_size=n_letters, hidden_size=n_hidden, output_size=n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "earned-workplace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7101, -3.7837, -3.7638, -3.6738, -3.7322, -3.8714, -3.8046, -3.7407,\n",
       "         -3.8662, -3.7178, -3.7269, -3.6650, -3.7664, -3.7766, -3.7517, -3.7422,\n",
       "         -3.8214, -3.6814, -3.7943, -3.7446, -3.6949, -3.7062, -3.8419, -3.6371,\n",
       "         -3.7452, -3.6865, -3.6187, -3.6388, -3.7371, -3.8577, -3.7310, -3.6975,\n",
       "         -3.7140, -3.7254, -3.7686, -3.6913, -3.7332, -3.7029, -3.7157, -3.7756,\n",
       "         -3.6985, -3.8031]], device='cuda:1', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for inference\n",
    "net = net.to(device)\n",
    "infer(net, \"kumar\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "latin-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7256, -3.7546, -3.7892, -3.6754, -3.6894, -3.8181, -3.7581, -3.7202,\n",
       "         -3.8300, -3.6722, -3.6850, -3.7424, -3.7674, -3.7925, -3.7973, -3.7730,\n",
       "         -3.7930, -3.6093, -3.8378, -3.7816, -3.6645, -3.7387, -3.8039, -3.6786,\n",
       "         -3.7691, -3.7115, -3.6383, -3.6441, -3.7763, -3.8337, -3.6468, -3.6908,\n",
       "         -3.6748, -3.7310, -3.8185, -3.6466, -3.7817, -3.7214, -3.7556, -3.8219,\n",
       "         -3.7053, -3.7970]], device='cuda:1', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = word_rep('A')\n",
    "\n",
    "# put stuff on GPU\n",
    "input = input.to(device)\n",
    "hidden = torch.zeros((1,1, n_hidden)).to(device)\n",
    "\n",
    "output, next_hidden = net(input, hidden)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "closing-stream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.7256, -3.7546, -3.7892, -3.6754, -3.6894, -3.8181, -3.7581, -3.7202,\n",
      "         -3.8300, -3.6722, -3.6850, -3.7424, -3.7674, -3.7925, -3.7973, -3.7730,\n",
      "         -3.7930, -3.6093, -3.8378, -3.7816, -3.6645, -3.7387, -3.8039, -3.6786,\n",
      "         -3.7691, -3.7115, -3.6383, -3.6441, -3.7763, -3.8337, -3.6468, -3.6908,\n",
      "         -3.6748, -3.7310, -3.8185, -3.6466, -3.7817, -3.7214, -3.7556, -3.8219,\n",
      "         -3.7053, -3.7970]], device='cuda:1', grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-3.7417, -3.7391, -3.7758, -3.6708, -3.6568, -3.7514, -3.8263, -3.7489,\n",
      "         -3.8795, -3.7125, -3.7390, -3.6674, -3.7828, -3.6888, -3.7516, -3.7476,\n",
      "         -3.8114, -3.5825, -3.7930, -3.7743, -3.6775, -3.7027, -3.7602, -3.7700,\n",
      "         -3.7316, -3.7220, -3.6404, -3.6993, -3.7385, -3.8266, -3.7213, -3.6682,\n",
      "         -3.7103, -3.7382, -3.7721, -3.6905, -3.8237, -3.7307, -3.6579, -3.8277,\n",
      "         -3.7440, -3.8662]], device='cuda:1', grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-3.6819, -3.7936, -3.7914, -3.6230, -3.7738, -3.7930, -3.8007, -3.7838,\n",
      "         -3.8044, -3.6951, -3.6763, -3.6510, -3.8428, -3.7512, -3.7488, -3.7570,\n",
      "         -3.7820, -3.6198, -3.7406, -3.7853, -3.6886, -3.7357, -3.8209, -3.6684,\n",
      "         -3.7108, -3.6488, -3.6490, -3.6905, -3.6998, -3.8608, -3.6922, -3.7284,\n",
      "         -3.6912, -3.7314, -3.7218, -3.7582, -3.7952, -3.7664, -3.6890, -3.8323,\n",
      "         -3.7402, -3.8481]], device='cuda:1', grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-3.7646, -3.7730, -3.7986, -3.6864, -3.7179, -3.8664, -3.7660, -3.7817,\n",
      "         -3.7883, -3.7097, -3.6801, -3.6703, -3.7928, -3.7749, -3.8181, -3.7810,\n",
      "         -3.8945, -3.6499, -3.7539, -3.7415, -3.6094, -3.7148, -3.7963, -3.6513,\n",
      "         -3.7288, -3.6601, -3.6171, -3.5824, -3.7841, -3.8322, -3.6344, -3.7188,\n",
      "         -3.6641, -3.6999, -3.7786, -3.6543, -3.8588, -3.8002, -3.7411, -3.8097,\n",
      "         -3.7553, -3.7927]], device='cuda:1', grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-3.7373, -3.7632, -3.7761, -3.6725, -3.7190, -3.8795, -3.7819, -3.7607,\n",
      "         -3.8387, -3.6998, -3.7306, -3.6843, -3.7790, -3.7892, -3.7592, -3.7402,\n",
      "         -3.8419, -3.6711, -3.8186, -3.7352, -3.6813, -3.7269, -3.7961, -3.6405,\n",
      "         -3.7422, -3.6719, -3.6296, -3.5955, -3.7515, -3.8774, -3.7063, -3.7124,\n",
      "         -3.6973, -3.7212, -3.7552, -3.6494, -3.8172, -3.6979, -3.7412, -3.7648,\n",
      "         -3.7166, -3.7948]], device='cuda:1', grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-3.7370, -3.8110, -3.7710, -3.6596, -3.7193, -3.8272, -3.7731, -3.8884,\n",
      "         -3.7653, -3.7264, -3.7765, -3.6107, -3.7879, -3.7061, -3.7567, -3.7502,\n",
      "         -3.8029, -3.6482, -3.8015, -3.7090, -3.6209, -3.7321, -3.7687, -3.5916,\n",
      "         -3.6854, -3.6805, -3.7042, -3.6738, -3.7563, -3.9156, -3.6669, -3.7009,\n",
      "         -3.6472, -3.7012, -3.7883, -3.7199, -3.8495, -3.7716, -3.7169, -3.7825,\n",
      "         -3.7335, -3.8522]], device='cuda:1', grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = word_rep('Albert')\n",
    "hidden = torch.zeros((1,1, n_hidden))\n",
    "\n",
    "# put stuff on GPU\n",
    "input = input.to(device)\n",
    "hidden = hidden.to(device)\n",
    "\n",
    "next_hidden = hidden\n",
    "for i in range(input.shape[0]):\n",
    "    output, next_hidden = net(input[i].reshape(1,1,-1), next_hidden)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-category",
   "metadata": {},
   "source": [
    "### 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "republican-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "for l in all_tags: \n",
    "    count[l] = 0\n",
    "for k,v in cat_words.items():\n",
    "    count[k] += len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "falling-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NNS': 75840, 'IN': 120996, 'VBP': 16158, 'VBN': 32328, 'NNP': 131426, 'TO': 23061, 'VB': 24211, 'DT': 98454, 'NN': 145807, 'CC': 23716, 'JJ': 78412, '.': 47831, 'VBD': 39379, 'WP': 2542, '``': 3728, 'CD': 24695, 'PRP': 13318, 'VBZ': 24960, 'POS': 11257, 'VBG': 19125, 'RB': 20252, ',': 32757, 'WRB': 2184, 'PRP$': 8655, 'MD': 6973, 'WDT': 3698, 'JJR': 2967, ':': 795, 'JJS': 3034, 'WP$': 99, 'RP': 2490, 'PDT': 147, 'NNPS': 2521, 'EX': 663, 'RBS': 296, 'LRB': 678, 'RRB': 679, '$': 1149, 'RBR': 1055, ';': 214, 'UH': 24, 'FW': 1}\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "distinct-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c7424af28>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt_ = sns.barplot(list(count.keys()), list(count.values()))\n",
    "plt_.set_xticklabels(plt_.get_xticklabels(), rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-customs",
   "metadata": {},
   "source": [
    "### 7. Dataloader & Evaluate Model\n",
    "\n",
    "Check whether it works before training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "affected-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(npoints, X_, y_):\n",
    "    \"\"\"Function to load the data\"\"\"\n",
    "    to_ret = []\n",
    "    for i in range(npoints):\n",
    "        index_ = np.random.randint(len(X_))\n",
    "        name, lang = X_[index_], y_[index_] #subset the data\n",
    "        to_ret.append((name, lang, word_rep(name), tag_rep(lang)))\n",
    "    \n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "quarterly-stock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Benshoofin',\n",
       "  'NNP',\n",
       "  tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]]]),\n",
       "  tensor([0])),\n",
       " ('Wednesday',\n",
       "  'NNP',\n",
       "  tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0.]]]),\n",
       "  tensor([0]))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataloader\n",
    "dataloader(2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "demanding-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, n_points, topk, X_, y_, device = device):\n",
    "    \"Evaluation function\"\n",
    "\n",
    "    net = net.eval().to(device)\n",
    "    data_ = dataloader(n_points, X_, y_)\n",
    "    correct = 0\n",
    "\n",
    "    #iterate\n",
    "    for name, language, name_ohe, lang_rep in data_:\n",
    "        \n",
    "        name_ohe = name_ohe.to(device)\n",
    "        lang_rep = lang_rep.to(device)\n",
    "        \n",
    "        \n",
    "\n",
    "        #get the output\n",
    "        output = infer(net, name, device)\n",
    "        val, indices = output.topk(topk) #get the top k values\n",
    "        indices = indices.to(device) #convert to devices\n",
    "        \n",
    "        if lang_rep in indices:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct/n_points\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "weird-congress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the evaluation function\n",
    "eval(net, 1000, 1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-colleague",
   "metadata": {},
   "source": [
    "### 8. Batching pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "administrative-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a batched name rep\n",
    "\n",
    "def batched_name_rep(names, max_word_size):\n",
    "    rep = torch.zeros(max_word_size, len(names), n_letters)\n",
    "    for name_index, name in enumerate(names):\n",
    "        for letter_index, letter in enumerate(name):\n",
    "            pos = all_letters.find(letter)\n",
    "            rep[letter_index][name_index][pos] = 1\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "special-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_char(name_reps):\n",
    "    name_reps = name_reps.view((-1, name_reps.size()[-1]))\n",
    "    for t in name_reps: \n",
    "        if torch.sum(t) == 0:\n",
    "            print('<pad>')\n",
    "        else:\n",
    "            index = t.argmax()\n",
    "            print(all_letters[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "broad-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_lang_rep(langs):\n",
    "    rep = torch.zeros([len(langs)], dtype=torch.long)\n",
    "    for index, lang in enumerate(langs):\n",
    "        rep[index] = tags.index(lang)\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "otherwise-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataloader\n",
    "def batched_dataloader(npoints, X_, y_, verbose=False, device = device):\n",
    "    names = []\n",
    "    langs = []\n",
    "    X_lengths = []\n",
    "    \n",
    "    for i in range(npoints):\n",
    "        index_ = np.random.randint(len(X_))\n",
    "        name, lang = X_[index_], y_[index_]\n",
    "        X_lengths.append(len(name))\n",
    "        names.append(name)\n",
    "        langs.append(lang)\n",
    "    max_length = max(X_lengths)\n",
    "    \n",
    "    names_rep = batched_name_rep(names, max_length).to(device)\n",
    "    langs_rep = batched_lang_rep(langs).to(device)\n",
    "    \n",
    "    padded_names_rep = torch.nn.utils.rnn.pack_padded_sequence(names_rep, X_lengths, enforce_sorted = False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(names_rep.shape, padded_names_rep.data.shape)\n",
    "        print('--')\n",
    "    \n",
    "    if verbose:\n",
    "        print(names)\n",
    "        print_char(names_rep)\n",
    "        print('--')\n",
    "    \n",
    "    if verbose:\n",
    "        print_char(padded_names_rep.data)\n",
    "        print('Lang Rep', langs_rep.data)\n",
    "        print('Batch sizes', padded_names_rep.batch_sizes)\n",
    "    \n",
    "    \n",
    "    return padded_names_rep.to(device), langs_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "smooth-minimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "I\n",
      "e\n",
      "v\n",
      "a\n",
      "o\n",
      "u\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "out_ = batched_name_rep(['Beau', 'Ivo'], 5)\n",
    "print_char(out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "offensive-ranch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 2, 57]) torch.Size([9, 57])\n",
      "--\n",
      "['to', 'billion']\n",
      "t\n",
      "b\n",
      "o\n",
      "i\n",
      "<pad>\n",
      "l\n",
      "<pad>\n",
      "l\n",
      "<pad>\n",
      "i\n",
      "<pad>\n",
      "o\n",
      "<pad>\n",
      "n\n",
      "--\n",
      "b\n",
      "t\n",
      "i\n",
      "o\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      "Lang Rep tensor([18, 32], device='cuda:1')\n",
      "Batch sizes tensor([2, 2, 1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.]], device='cuda:1'), batch_sizes=tensor([2, 2, 1, 1, 1, 1, 1]), sorted_indices=tensor([1, 0], device='cuda:1'), unsorted_indices=tensor([1, 0], device='cuda:1')),\n",
       " tensor([18, 32], device='cuda:1'))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_dataloader(2, X_train, y_train, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-norman",
   "metadata": {},
   "source": [
    "### 10. Training\n",
    "#### 10.1 Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "flush-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic train function\n",
    "\n",
    "def train(net, opt, criterion, n_points):\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    total_loss = 0\n",
    "    \n",
    "    data_ = dataloader(n_points, X_train, y_train)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for name, language, name_ohe, lang_rep in data_:\n",
    "        \n",
    "        hidden = net.init_hidden()\n",
    "\n",
    "        for i in range(name_ohe.size()[0]):\n",
    "            output, hidden = net(name_ohe[i:i+1], hidden)\n",
    "        loss = criterion(output, lang_rep)\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "    opt.step()       \n",
    "    return total_loss/n_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "inside-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net, opt, criterion, n_points, device = device):\n",
    "    \n",
    "    net.train().to(device)\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    batch_input, batch_groundtruth = batched_dataloader(n_points, X_train, y_train, False, device)\n",
    "    batch_input = batch_input.to(device)\n",
    "    batch_groundtruth = batch_groundtruth.to(device)\n",
    "    \n",
    "    output, hidden = net(batch_input)\n",
    "    \n",
    "    loss = criterion(output, batch_groundtruth)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-executive",
   "metadata": {},
   "source": [
    "#### 10.2 Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "large-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNN_net(n_letters, n_hidden, n_tags) #.to(device)\n",
    "criterion = nn.NLLLoss().to(device)\n",
    "opt = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-blackjack",
   "metadata": {},
   "source": [
    "#### 10.4 Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "worse-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.92 s, sys: 368 ms, total: 5.29 s\n",
      "Wall time: 1.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.6930, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "#time for normal training\n",
    "train(net, opt, criterion, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-naples",
   "metadata": {},
   "source": [
    "### 11. Full training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "loving-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_setup(net, lr = 0.01, n_batches = 100, batch_size = 10, momentum = 0.9, display_freq=5, device = 'cpu'):\n",
    "    net = net.to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    opt = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    loss_arr = np.zeros(n_batches + 1)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        loss_arr[i+1] = (loss_arr[i]*i + train_batch(net, opt, criterion, batch_size, device))/(i + 1)\n",
    "        \n",
    "        if i%display_freq == display_freq-1:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print('Iteration', i, 'Loss', loss_arr[i])\n",
    "            # print('Top-1:', eval(net, len(X_test), 1, X_test, y_test), 'Top-2:', eval(net, len(X_test), 2, X_test, y_test))\n",
    "            plt.figure()\n",
    "            plt.plot(loss_arr[1:i], '-*')\n",
    "            plt.xlabel('Iteration')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.show()\n",
    "            print('\\n\\n')\n",
    "            \n",
    "    print('Top-1 Accuracy:', eval(net, len(X_test), 1, X_test, y_test, device), 'Top-2 Accuracy:', eval(net, len(X_test), 2, X_test, y_test, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#training RNN using batch technique\n",
    "net = RNN_net(n_letters, 128, n_tags)\n",
    "train_setup(net, lr=0.15, n_batches=3200, batch_size = 512, display_freq=500) # CPU Training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-plumbing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
