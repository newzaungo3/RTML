{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# LSTM Lab\n",
    "\n",
    "To be done on your own: find another dataset of sentence pairs in a different domain and see if you can preprocess the data and train a chatbot model on it using the same code we developed today. Report your results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from tqdm.contrib.discord import tqdm, trange\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word tokens\n",
    "\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "\n",
    "    def __init__(self):        \n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a Voc object\n",
    "\n",
    "def readVocs(datafile):\n",
    "    print(\"Reading lines...\")    \n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc()\n",
    "    return voc, pairs\n",
    "\n",
    "# Boolean function returning True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using the filterPair predicate\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "\n",
    "def loadPrepareData(datafile):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 64271 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 18008\n",
      "\n",
      "pairs:\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "# wget https://github.com/dsai-asia/RTML/raw/main/Labs/11-LSTMs/chatDataset.txt\n",
    "\n",
    "# Load/Assemble Voc and pairs\n",
    "\n",
    "datafile = './data/formatted_movie_lines.txt'\n",
    "voc, pairs = loadPrepareData(datafile)\n",
    "\n",
    "# Print some pairs to validate\n",
    "\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "keep_words 7823 / 18005 = 0.4345\nTrimmed from 64271 pairs to 53165, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "# Trim vocabulary and pairs\n",
    "\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['yeah .', 'what have you got ?'], ['so one of them got to him .', 'maybe .'], ['maybe .', 'you know who ?'], ['you know who ?', 'maybe .'], ['who else knows ?', 'just us .'], ['who else knows ?', 'just us .'], ['it s all over paul .', 'move ! or you die right here !'], ['they kid a lot .', 'i would not be too sure .'], ['where are you taking us now ?', 'home .'], ['home .', 'you couldn t wait until morning ?'], ['you said we would be safe in philadelphia .', 'i was wrong .'], ['no no doctor . . .', 'but why ?'], ['does anybody know i m here ?', 'only the elders .'], ['only the elders .', 'how long ?'], ['how long ?', 'what ?'], ['what ?', 'how long have i been here ?'], ['how long have i been here ?', 'two days .'], ['tell him his tea stinks .', 'you tell him . when you re able .'], ['something wrong with buttons ?', 'buttons are hochmut .'], ['buttons are hochmut .', 'hochmut ?'], ['the . . . bullets ?', 'oh . the bullets .'], ['when will you be going ?', 'not long . . . a few days .'], ['i know .', 'i was being foolish ?'], ['you re so sure of that ?', 'aren t you ? after today ?'], ['that s your way not mine .', 'that s god s way !'], ['anybody know about this ?', 'i didn t even know about it .'], ['it s me .', 'johnny ! where the hell have you been ?'], ['where i m at is maybe .', 'say again ?'], ['say again ?', 'make that .'], ['you know where he is .', 'wrong .'], ['wrong .', 'you d lie to protect him .'], ['you d lie to protect him .', 'probably .'], ['you re the first one he ll contact .', 'he s got my number .'], ['did you find him ?', 'not yet .'], ['everything okay ?', 'yes thank you very much .'], ['yes thank you very much .', 'john said you re amish .'], ['john said you re amish .', 'yes .'], ['yes .', 'oh .'], ['good morning .', 'you didn t have to . . .'], ['who was that man ?', 'his name is john book .'], ['is the english dead ?', 'no . . .'], ['no . . .', 'looks dead . . .'], ['that has no place in this house .', 'i know .'], ['i am not a child .', 'you are acting like one !'], ['you are acting like one !', 'i will be the judge of that .'], ['no rachel . . .', 'i have to help him !'], ['she say where he is ?', 'i don t think she knows .'], ['what about carter ?', 'tight . but i m working on him .'], ['tight . but i m working on him .', 'lean on him .'], ['he s not in this building .', 'all right where is he ?'], ['he ll live .', 'you might have killed him !'], ['get back in there .', 'my son is out there !'], ['i don t want to stay here .', 'they are english . they don t understand .'], ['aunt em !', 'fifty seven fifty eight'], ['dorothy ! dorothy ! we re busy !', 'oh all right .'], ['yes .', 'oh i hope we got them in time .'], ['oh i hope we got them in time .', 'yes .'], ['to see ?', 'if she . . .'], ['if she . . .', 'if she ?'], ['. . . oz has spoken !', 'who are you ?'], ['oh', 'goodbye folks !'], ['oh !', 'who did it ? now wait a minute .'], ['run toto run !', 'catch him you fool !'], ['toto too .', 'oh now ?'], ['oh now ?', 'whenever you wish .'], ['are you ready now ?', 'yes . say goodbye toto .'], ['aw come come come', 'no they won t honestly .'], ['no they won t honestly .', 'oh'], ['that s our farm !', 'oh yes .'], ['yes . . .that s aunt em .', 'her her name is emily .'], ['uh huh .', 'what s she doing now ?'], ['oh no no !', 'that s all the crystal s gone dark .'], ['that s all the crystal s gone dark .', 'oh you . . . .'], ['oh', 'but you couldn t have been could you ?'], ['now you re seeing reason .', 'no'], ['did you say something ?', 'oil can . . . .'], ['oh', '. . .oh did that hurt ?'], ['oh', 'all hollow . oh'], ['oh i ll get it !', 'oh ! oh !'], ['we might .', 'oh'], ['and bears !', 'what sort of an animal is that ?'], ['no . why only oh', 'oh ! oh tin man ! oh !'], ['oh ! something bit me too !', 'now come on you re acting silly'], ['dorothy !', 'i knew you would !'], ['oh my !', 'lions and tigers and bears !'], ['lions and tigers and bears !', 'oh my !'], ['oh my !', 'lions and tigers and bears !'], ['lions and tigers and bears !', 'oh my !'], ['oh my !', 'lions and tigers and bears !'], ['lions and tigers and bears !', 'oh my !'], ['oh my !', 'lions and tigers and bears !'], ['lions and tigers and bears !', 'oh my !'], ['a home', 'the nerve .'], ['yes let s run !', 'yes .'], ['ho ho ho ho', 'hah !'], ['i d sooner wait outside .', 'but why ? why ?'], ['but why ? why ?', 'because i m still scared !'], ['because i m still scared !', 'oh come on .'], ['oh come on .', 'ohh !'], ['oh oh come on .', 'huh ? what d he say ?'], ['does it work ?', 'no but it s wonderful for threatening with .'], ['no but it s wonderful for threatening with .', 'oh'], ['they re coming back !', 'ohhh !'], ['. . .don t talk .', 'it s pleasant down that way too .'], ['. . .yes .', 'ohhhh'], ['ohhhh', 'ohh !'], ['oh ! oh ! ohhh !', 'did i scare you ?'], ['they would ?', 'um hmm .'], ['um hmm .', 'where s kansas ?'], ['oh i ll try ! really i will .', 'to oz ?'], ['to oz ?', 'to oz !'], ['what ?', 'yes . oh look'], ['oil can what ?', 'oil can ? oh oh here it is !'], ['he said his mouth .', 'here here'], ['here here', 'the other side . . . .'], ['the other side . . . .', 'yes there .'], ['oh oh', 'oh are you are you all right ?'], ['oh come on now everybody', 'did you just hear what i just heard ?'], ['no no no no !', 'ohh ! ohh ! ohh !'], ['yeah .', 'ah'], ['ah', 'dorothy next !'], ['who at ?', 'i don t know .'], ['what happened ?', 'somebody pulled my tail .'], ['somebody pulled my tail .', 'oh you did it yourself !'], ['oh you did it yourself !', 'i oh'], ['i oh', 'here come on .'], ['here come on .', 'what was that ?'], ['fine . he s got a plan', 'and you re going to lead us .'], ['and you re going to lead us .', 'yeah . me ?'], ['yeah . me ?', 'yes you .'], ['up !', 'now . . .'], ['where where do we go now ?', 'this way ! come on !'], ['i haven t slept in weeks .', 'why don t you try counting sheep ?'], ['it s a whatzis .', 'it s a whatzis ?'], ['it s a whatzis ?', 'whozat ?'], ['whozat ?', 'whozat ?'], ['no ? now wait a minute .', 'you don t neither'], ['where do we go now ?', 'yeah .'], ['. . . about dorothy .', 'dorothy ? well what has dorothy done ?'], ['you mean she bit you ?', 'no her dog !'], ['no her dog !', 'oh she bit her dog eh ?'], ['oh she bit her dog eh ?', 'no !'], ['no sir !', 'no sir !'], ['that s right .', 'we do .'], ['we do .', 'to oz ?'], ['to oz ?', 'to oz !'], ['animals that that eat straw ?', 'some but mostly lions and tigers and bears .'], ['then i m sure to get a brain', 'a heart'], ['come on come on', 'hurry hurry'], ['hey dorothy !', 'dorothy !'], ['oh oh poor dorothy !', 'don t cry you ll rust yourself again !'], ['let s .', 'yes .'], ['ha ha ha', 'ho ho ho'], ['oh did did you see that ?', 'oh look out .'], ['oh look out .', 'you know something ?'], ['now now don t fret .', 'oh dear dear .'], ['oh dear dear .', 'we ll get you together !'], ['. . .to dorothy !', 'oh'], ['oh', 'come on fellows !'], ['no you don t .', 'oh no !'], ['you put up a great fight lion .', 'yeah'], ['oh upstairs quickly !', 'go on !'], ['hey what about dorothy ?', 'yes how about dorothy ?'], ['it makes you nervous ?', 'yes .'], ['what about us ?', 'well i'], ['but i still want one .', 'yes . . . .'], ['ruined my exit !', 'help !'], ['so who are you ?', 'bond'], ['makes her look even more innocent .', 'what would she want with weapons grade plutonium ?'], ['what would she want with weapons grade plutonium ?', 'i was hoping you could tell me .'], ['is there another way ?', 'we go down to the torpedo bay .'], ['do you know what you re doing ?', 'like riding a bike .'], ['but what if . . .', 'count to twenty . i ll be there .'], ['we ve got to get out .', 'we can t .'], ['perhaps after this . . . test ?', '. . .yes ? . . .'], ['. . .yes ? . . .', 'i could come for a second opinion ?'], ['construction s not exactly my line .', 'quite the opposite in fact .'], ['the inside man ?', 'is maybe . . .the inside woman .'], ['your last chance . take the money .', 'your last chance . give me the name .'], ['beautiful isn t it ?', 'yes .'], ['he was my father .', 'i m sorry .'], ['i met you at my father s funeral .', 'yes .'], ['how long has he been with you ?', 'since the kidnapping . why do you ask ?'], ['i can t stay here .', 'you re not going to .'], ['no ! it will cave in !', 'it s the only way out . . .'], ['i can t stay .', 'i know .'], ['what is it ?', '. . .you should rest .'], ['james . . .', 'i have to go .'], ['i have to go .', 'then take me with you .'], ['then take me with you .', 'no . you ll be safe here .'], ['no . you ll be safe here .', 'i don t want to be safe !'], ['i don t want to be safe !', 'i have to go to work .'], ['i could have given you the world .', 'not interested .'], ['where s m ?', 'soon she ll be everywhere .'], ['now . . .where were we ?', 'a rope !'], ['where s the sub going ? !', 'no ! get me out !'], ['where ?', 'istanbul .'], ['the maiden s tower .', 'how appropriate .'], ['no ! bond !', 'bond !'], ['what class sub does your nephew run ?', 'c class .'], ['perhaps this isn t the time . . .', 'please .'], ['someone will come .', 'who ? bond ? bond is dead .'], ['warm .', 'is it ?'], ['so beautiful . so smooth so warm .', 'how would you know ?'], ['he was a . . .good lover ?', 'what do you think ?'], ['the parcel s in the post .', 'it s heading for the oil terminal .'], ['it s heading for the oil terminal .', 'where it can do the most damage .'], ['any word from him ?', 'still no contact yet .'], ['what ? !', 'please .'], ['walter', 'don t call me that .'], ['a whore fucking .', 'who is she ? do you know her ?'], ['god isn t there to do it .', 'we don t know that .'], ['hi jon .', 'hello adrian .'], ['would you like me to stay ?', 'mm hmm .'], ['mm hmm .', 'i could stay and go .'], ['what am i', 'cured yes .'], ['and what if you re wrong ?', 'i m not .'], ['i m not .', 'what if you re wrong ? ?'], ['his what ?', 'his . . . whatever .'], ['. . . what ?', 'pick a record . i feel like dancing .'], ['god you sound like jon . turn around .', 'what are you up to ?'], ['what are you up to ?', 'don t look . turn around .'], ['they re following us .', 'all right . mission accomplished .'], ['they re practically on us . . .', 'that detroit shit ? i m so worried .'], ['what s he doing ?', 'i think he s going to the john .'], ['he saved himself . he changed the past .', 'where did he go ?'], ['dan is this is this new york ? ?', 'those cars . what year is this ? ?'], ['those cars . what year is this ? ?', 'everything s changed'], ['what do you mean frame up ?', 'obvious pattern all ties in'], ['we should get back what ?', 'nothing . what were you going to say ?'], ['i can fix it .', 'what ?'], ['oh sweetheart just a quick one .', 'no . we laid out very careful ground rules'], ['their real names please .', 'i don t know their real names .'], ['i don t know their real names .', 'you re lying miss juspeczyk .'], ['you re lying miss juspeczyk .', 'i don t know their real names !'], ['i m terribly sorry .', 'what does this mean ? what does it'], ['all right miss juspeczyk . pack your things .', 'am i free to go ?'], ['adrian .', 'laurie ! good to see you .'], ['adrian ! don t leave so soon . i', 'i ll take a raincheck laurie .'], ['i ll take a raincheck laurie .', 'please .'], ['maybe i ll do that .', 'i m sorry about blake .'], ['new information .', 'ever see one of these before ?'], ['what is this place ?', 'looks like a diner .'], ['is that all ?', 'just remember i ll be watching .'], ['like what ?', 'he gets to be the hero .'], ['yorgi asked me to .', 'you do everything yorgi says ?'], ['you do everything yorgi says ?', 'go to hell .'], ['who are you ?', 'we hung out last night remember ?'], ['they tell me you re an american agent .', 'what are you talking about ?'], ['it doesn t matter anymore forget it .', 'of course it matters . hey hold on !'], ['we ll have to go after them .', 'aren t you afraid ?'], ['where d the damn truck go ? !', 'go to the water it s that way !'], ['he s shut out the communication circuit !', 'you can t talk to it ?'], ['the peace conference . . .', 'nice place to start don t you think ?'], ['i ll leave you two alone to talk .', 'yeah thanks a lot .'], ['sorry dude .', 'what s the deal ?'], ['i wish i could go .', 'we all do .'], ['scott . . i', 'i don t like him being here .'], ['i don t like him being here .', 'what are you talking about ?'], ['well i think that s perfectly understandable .', 'i ll tell you one thing though .'], ['i ll tell you one thing though .', 'what s that ?'], ['into the hudson ?', 'uh huh .'], ['anything else i can get you ?', 'some cigars . case of beer .'], ['maybe the professor could help you with that .', 'by reading my thoughts ?'], ['by reading my thoughts ?', 'if necessary .'], ['were you now ?', 'what do you say ?'], ['what do you say ?', 'not interested .'], ['let go .', 'suit yourself .'], ['he takes his work seriously .', 'he takes himself seriously .'], ['logan do you see this ring ?', 'i ve seen a lot of rings .'], ['i ve seen a lot of rings .', 'yeah i ll bet you have .'], ['dr . grey .', 'senator ?'], ['i do love a good check mate .', 'what do you want ?'], ['where are we going ?', 'to find rogue .'], ['to find rogue .', 'how ?'], ['you designed this yourself ?', 'actually magneto helped me put it together .'], ['actually magneto helped me put it together .', 'he helped you ?'], ['i lost him .', 'how ?'], ['how ?', 'it was xavier s people . they knew .'], ['is that what you re looking for ?', 'a piece . only a piece .'], ['a piece . only a piece .', 'is it enough ?'], ['is it enough ?', 'enough for a test .'], ['i ll find him .', 'alive .'], ['frederick frankenstein ?', 'you have the wrong house .'], ['you have the wrong house .', 'and who might you be ?'], ['and who might you be ?', 'dr . frederick fronkonsteen .'], ['dr . frederick fronkonsteen .', 'the grandson of victor fronkonsteen ?'], ['the grandson of victor fronkonsteen ?', 'no !'], ['no !', 'what was your grandfather s name ?'], ['what was your grandfather s name ?', 'victor frankenstein .'], ['give him an extra dollar .', 'yes sir .'], ['carlson !', 'yes sir ?'], ['what in god s name are you doing ?', 'baack !'], ['baack !', 'what ?'], ['what ?', 'baack !'], ['mmmmm !', 'is it that music ?'], ['is it that music ?', 'mmmmm ! mmmmm !'], ['still happy you married me ?', 'mmmmm .'], ['mmmmm .', 'love me oodles and oodles ?'], ['love me oodles and oodles ?', 'mmmmm .'], ['darling !', 'hello . . . ?'], ['hello . . . ?', 'surprised ?'], ['surprised ?', 'well . . . yes .'], ['well . . . yes .', 'miss me ?'], ['miss me ?', 'i . . .'], ['of course .', 'you have your tickets ?'], ['you have your tickets ?', 'yes .'], ['yes .', 'and your passport ?'], ['and your passport ?', 'yes don t worry .'], ['yes .', 'promise ? ?'], ['promise ? ?', 'i promise .'], ['oh nice .', 'i hope you like large weddings .'], ['i hope you like large weddings .', 'whatever makes you happy .'], ['does that mean you love me ?', 'you bet your boots it does .'], ['i will ! goodbye darling !', 'goodbye darling .'], ['goodbye darling .', 'goodbye darling !'], ['darling !', 'darling !'], ['surprised ?', 'surprised !'], ['surprised !', 'love me ?'], ['that s a tough choice .', 'is it worth taking a chance ?'], ['is it worth taking a chance ?', 'i suppose you re right .'], ['good night .', 'that s my good boy .'], ['well . . . we ll see .', 'will there be anything else ?'], ['dr . frankenstein ?', 'fron kon steen !'], ['how long will this whole thing take ?', 'a week . ten days at most .'], ['why did you do that ?', 'what ?'], ['what ?', 'break that old man s violin .'], ['break that old man s violin .', 'i didn t do that .'], ['yes sir .', 'one week at the most ! ?'], ['one week at the most ! ?', 'one week i ll see to it sir .'], ['that s fronkonsteen .', 'i beg your pardon ?'], ['i beg your pardon ?', 'my name is pronounced fron kon steen .'], ['dr . fronkonsteen !', 'yes ?'], ['how old are you young man ?', 'nineteen sir .'], ['frederick frankenstein ?', 'fron kon steen !'], ['fron kon steen !', 'are you putting me on ?'], ['are you putting me on ?', 'no it s pronounced fron kon steen .'], ['are these your bags ?', 'yes just the two .'], ['there .', 'i beg your pardon ?'], ['i beg your pardon ?', 'there wolf ! there castle !'], ['there wolf ! there castle !', 'why are you talking like that ?'], ['why are you talking like that ?', 'i thought you wanted to .'], ['i thought you wanted to .', 'no .'], ['what wockers ?', 'the wockers with the knockers .'], ['the wockers with the knockers .', 'wockers with the knockers ? ? ?'], ['home !', 'home .'], ['but what you were doing ?', 'just putting up some tea .'], ['just putting up some tea .', 'did you hear that strange music ?'], ['did you hear that strange music ?', 'what ?'], ['what ?', 'did you hear that strange music ? ?'], ['did you hear that strange music ? ?', 'what ?'], ['what ?', 'did you hear that strange music ?'], ['what is this place ?', 'must be the music room .'], ['what a filthy job !', 'could be worse !'], ['could be worse !', 'how ?'], ['how ?', 'could be raining !'], ['i want that brain .', 'was he any good ?'], ['that s not bad .', 'can you imagine that brain in this body ?'], ['oh ! may i call you master ?', 'why ?'], ['if you like just hurry !', 'thank you master .'], ['throw the second switch !', 'this guy means business .'], ['what ?', 'more do you hear me ?'], ['more do you hear me ?', 'what ?'], ['what ?', 'throw the third switch !'], ['throw the third switch !', 'wait till he sees the bill .'], ['but you did i just heard it .', 'it wasn t me .'], ['original .', 'give me your hand !'], ['you can say that again .', 'yes .'], ['yes master !', 'act casual !'], ['how s it going ?', 'what did you find out ?'], ['what did you find out ?', 'someone was playing this in the music room .'], ['where is he ?', 'how do you know it was a he ?'], ['how do you know it was a he ?', 'all right where is she ?'], ['all right where is she ?', 'how do you know it was a she ?'], ['how do you know it was a she ?', 'bring me the violin !'], ['bring me the violin !', 'can you play it ?'], ['thanks . . .for all your help .', 'that s what we re paid for .'], ['come on big fellow !', 'is everything ready ?'], ['get the sedative ready !', 'mmmmm ! mmmmm !'], ['nice ! nice little balance to it .', 'ja ja .'], ['extremely well .', 'how nice .'], ['did you have a pleasant trip ?', 'yes thank you . it wasn t bad .'], ['ooh !', 'what a filthy mess .'], ['look doctor !', 'well this explains the music .'], ['well this explains the music .', 'but who was playing it ?'], ['oh doctor !', 'perhaps we d better leave .'], ['a god ?', 'yes !'], ['yes !', 'i know .'], ['reputation . reputation !', 'i thought it was wonderful .'], ['oh doctor !', 'i think you ve done it master .'], ['it looks that way .', 'what do you think we should do ?'], ['do you think you can sing it ?', 'me ? sing ?'], ['me ? sing ?', 'yes quickly dear !'], ['all right give it to him !', 'are you serious ? ?'], ['are you serious ? ?', 'give him the sedative !'], ['give him the sedative !', 'oh ! yes doctor .'], ['did you do it ?', 'i think so .'], ['good night doctor .', 'good night !'], ['it wouldn t be fair to elizabeth .', 'of course not .'], ['nor to elizabeth .', 'no . nor to elizabeth .'], ['and elizabeth has hers .', 'yes elizabeth has hers .'], ['yes elizabeth has hers .', 'but after all you have yours .'], ['yes i have mine .', 'and i have mine .'], ['yes . . .yes you have yours .', 'why don t we talk inside ?'], ['yes i know .', 'it wouldn t be fair to her .'], ['it wouldn t be fair to her .', 'yes i know .'], ['how long is it so far ?', 'four'], ['four', 'three minutes to go !'], ['three minutes to go !', 'yes .'], ['another fifteen seconds to go .', 'do something ! stall them !'], ['yes sir name please ?', 'food !'], ['food !', 'do you have a reservation ?'], ['do you have a reservation ?', 'food ! !']]\n"
     ]
    }
   ],
   "source": [
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpairs = pairs[45000:]\n",
    "pairs  = pairs[:45000]"
   ]
  },
  {
   "source": [
    "# LSTM model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NaiveCustomLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        # Parameters for computing i_t\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing f_t\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing c_t\n",
    "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing o_t\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "         \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        forward: Run input x through the cell. Assumes x.shape is (batch_size, sequence_length, input_size)\n",
    "        \"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        \n",
    "        # Reshape hidden_seq tensor to (batch size, sequence length, hidden_size)\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Return a padded input sequence tensor and the lengths of each original sequence\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Return a padded target sequence tensor, a padding mask, and the max target length\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Return all items for a given batch of pairs\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# Example for validation\n",
    "\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['there .', 'where ?'], ['you have my word . as a gentleman', 'you re sweet .'], ['hi .', 'looks like things worked out tonight huh ?'], ['have fun tonight ?', 'tons'], ['well no . . .', 'then that s all you had to say .']]\n[['you have my word . as a gentleman', 'you re sweet .'], ['well no . . .', 'then that s all you had to say .'], ['have fun tonight ?', 'tons'], ['there .', 'where ?'], ['hi .', 'looks like things worked out tonight huh ?']]\ntensor([[ 124,   98,  100,    7,  383],\n        [   4,  147,    6,   80,    7],\n        [ 167,  582,    2,    6, 4036],\n        [   4,    6,    0,  659,    2],\n        [   2,    2,    0,    6,    0],\n        [   0,    0,    0,    2,    0]])\ntensor([[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 0, 1, 1],\n        [1, 1, 0, 1, 0],\n        [0, 0, 0, 1, 0]], dtype=torch.uint8)\n6\n"
     ]
    }
   ],
   "source": [
    "pair_batch = pairs[:5]\n",
    "print(pair_batch)\n",
    "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "print(pair_batch)\n",
    "print(target_variable)\n",
    "print(mask)\n",
    "print(max_target_len)"
   ]
  },
  {
   "source": [
    "## Encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu())\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        \n",
    "        return outputs, hidden\n",
    "    def init_hidden(self):\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "       \n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))#, bidirectional=True)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "  \n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    \n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    \n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    \n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "max_target_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    losslist = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for iteration in tqdm(range(start_iteration, n_iteration + 1),token='ODI3MTkyMDYyNzU0ODE2MDIy.YGXcpA.nAg3mOoCUlkFdG5GPH83EJrvrcs', channel_id='827196085738536971'):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        \n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        \n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            losslist.append(print_loss_avg)\n",
    "        \n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            print(directory)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "    return losslist"
   ]
  },
  {
   "source": [
    "## Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "       \n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            \n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        \n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "  \n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            \n",
    "            input_sentence = input('> ')\n",
    "            \n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            \n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            \n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            \n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/6000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5f27b8a819143208db4359ba817c52d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "7.3%; Average loss: 2.9422\n",
      "Iteration: 2850; Percent complete: 47.5%; Average loss: 2.9295\n",
      "Iteration: 2860; Percent complete: 47.7%; Average loss: 2.9610\n",
      "Iteration: 2870; Percent complete: 47.8%; Average loss: 2.9469\n",
      "Iteration: 2880; Percent complete: 48.0%; Average loss: 2.9083\n",
      "Iteration: 2890; Percent complete: 48.2%; Average loss: 2.8852\n",
      "Iteration: 2900; Percent complete: 48.3%; Average loss: 2.9359\n",
      "Iteration: 2910; Percent complete: 48.5%; Average loss: 2.9076\n",
      "Iteration: 2920; Percent complete: 48.7%; Average loss: 2.9001\n",
      "Iteration: 2930; Percent complete: 48.8%; Average loss: 2.8610\n",
      "Iteration: 2940; Percent complete: 49.0%; Average loss: 2.9083\n",
      "Iteration: 2950; Percent complete: 49.2%; Average loss: 2.9060\n",
      "Iteration: 2960; Percent complete: 49.3%; Average loss: 2.9403\n",
      "Iteration: 2970; Percent complete: 49.5%; Average loss: 2.9107\n",
      "Iteration: 2980; Percent complete: 49.7%; Average loss: 2.9125\n",
      "Iteration: 2990; Percent complete: 49.8%; Average loss: 2.8469\n",
      "Iteration: 3000; Percent complete: 50.0%; Average loss: 2.8539\n",
      "Iteration: 3010; Percent complete: 50.2%; Average loss: 2.8858\n",
      "Iteration: 3020; Percent complete: 50.3%; Average loss: 2.9008\n",
      "Iteration: 3030; Percent complete: 50.5%; Average loss: 2.8803\n",
      "Iteration: 3040; Percent complete: 50.7%; Average loss: 2.8834\n",
      "Iteration: 3050; Percent complete: 50.8%; Average loss: 2.8781\n",
      "Iteration: 3060; Percent complete: 51.0%; Average loss: 2.8714\n",
      "Iteration: 3070; Percent complete: 51.2%; Average loss: 2.8519\n",
      "Iteration: 3080; Percent complete: 51.3%; Average loss: 2.8694\n",
      "Iteration: 3090; Percent complete: 51.5%; Average loss: 2.8776\n",
      "Iteration: 3100; Percent complete: 51.7%; Average loss: 2.8646\n",
      "Iteration: 3110; Percent complete: 51.8%; Average loss: 2.8534\n",
      "Iteration: 3120; Percent complete: 52.0%; Average loss: 2.8512\n",
      "Iteration: 3130; Percent complete: 52.2%; Average loss: 2.7982\n",
      "Iteration: 3140; Percent complete: 52.3%; Average loss: 2.8340\n",
      "Iteration: 3150; Percent complete: 52.5%; Average loss: 2.8177\n",
      "Iteration: 3160; Percent complete: 52.7%; Average loss: 2.8354\n",
      "Iteration: 3170; Percent complete: 52.8%; Average loss: 2.8027\n",
      "Iteration: 3180; Percent complete: 53.0%; Average loss: 2.7872\n",
      "Iteration: 3190; Percent complete: 53.2%; Average loss: 2.8309\n",
      "Iteration: 3200; Percent complete: 53.3%; Average loss: 2.8148\n",
      "Iteration: 3210; Percent complete: 53.5%; Average loss: 2.8296\n",
      "Iteration: 3220; Percent complete: 53.7%; Average loss: 2.7988\n",
      "Iteration: 3230; Percent complete: 53.8%; Average loss: 2.7876\n",
      "Iteration: 3240; Percent complete: 54.0%; Average loss: 2.7744\n",
      "Iteration: 3250; Percent complete: 54.2%; Average loss: 2.7836\n",
      "Iteration: 3260; Percent complete: 54.3%; Average loss: 2.7559\n",
      "Iteration: 3270; Percent complete: 54.5%; Average loss: 2.7600\n",
      "Iteration: 3280; Percent complete: 54.7%; Average loss: 2.7998\n",
      "Iteration: 3290; Percent complete: 54.8%; Average loss: 2.7476\n",
      "Iteration: 3300; Percent complete: 55.0%; Average loss: 2.7584\n",
      "Iteration: 3310; Percent complete: 55.2%; Average loss: 2.7564\n",
      "Iteration: 3320; Percent complete: 55.3%; Average loss: 2.7501\n",
      "Iteration: 3330; Percent complete: 55.5%; Average loss: 2.7643\n",
      "Iteration: 3340; Percent complete: 55.7%; Average loss: 2.7624\n",
      "Iteration: 3350; Percent complete: 55.8%; Average loss: 2.7552\n",
      "Iteration: 3360; Percent complete: 56.0%; Average loss: 2.7509\n",
      "Iteration: 3370; Percent complete: 56.2%; Average loss: 2.7841\n",
      "Iteration: 3380; Percent complete: 56.3%; Average loss: 2.7519\n",
      "Iteration: 3390; Percent complete: 56.5%; Average loss: 2.7428\n",
      "Iteration: 3400; Percent complete: 56.7%; Average loss: 2.6996\n",
      "Iteration: 3410; Percent complete: 56.8%; Average loss: 2.7197\n",
      "Iteration: 3420; Percent complete: 57.0%; Average loss: 2.7219\n",
      "Iteration: 3430; Percent complete: 57.2%; Average loss: 2.7454\n",
      "Iteration: 3440; Percent complete: 57.3%; Average loss: 2.7067\n",
      "Iteration: 3450; Percent complete: 57.5%; Average loss: 2.7073\n",
      "Iteration: 3460; Percent complete: 57.7%; Average loss: 2.7058\n",
      "Iteration: 3470; Percent complete: 57.8%; Average loss: 2.7081\n",
      "Iteration: 3480; Percent complete: 58.0%; Average loss: 2.7155\n",
      "Iteration: 3490; Percent complete: 58.2%; Average loss: 2.6952\n",
      "Iteration: 3500; Percent complete: 58.3%; Average loss: 2.7153\n",
      "Iteration: 3510; Percent complete: 58.5%; Average loss: 2.7190\n",
      "Iteration: 3520; Percent complete: 58.7%; Average loss: 2.6772\n",
      "Iteration: 3530; Percent complete: 58.8%; Average loss: 2.6901\n",
      "Iteration: 3540; Percent complete: 59.0%; Average loss: 2.6758\n",
      "Iteration: 3550; Percent complete: 59.2%; Average loss: 2.6691\n",
      "Iteration: 3560; Percent complete: 59.3%; Average loss: 2.6670\n",
      "Iteration: 3570; Percent complete: 59.5%; Average loss: 2.6582\n",
      "Iteration: 3580; Percent complete: 59.7%; Average loss: 2.6808\n",
      "Iteration: 3590; Percent complete: 59.8%; Average loss: 2.6810\n",
      "Iteration: 3600; Percent complete: 60.0%; Average loss: 2.6791\n",
      "Iteration: 3610; Percent complete: 60.2%; Average loss: 2.6800\n",
      "Iteration: 3620; Percent complete: 60.3%; Average loss: 2.6454\n",
      "Iteration: 3630; Percent complete: 60.5%; Average loss: 2.6556\n",
      "Iteration: 3640; Percent complete: 60.7%; Average loss: 2.6616\n",
      "Iteration: 3650; Percent complete: 60.8%; Average loss: 2.6387\n",
      "Iteration: 3660; Percent complete: 61.0%; Average loss: 2.6335\n",
      "Iteration: 3670; Percent complete: 61.2%; Average loss: 2.6721\n",
      "Iteration: 3680; Percent complete: 61.3%; Average loss: 2.6410\n",
      "Iteration: 3690; Percent complete: 61.5%; Average loss: 2.6306\n",
      "Iteration: 3700; Percent complete: 61.7%; Average loss: 2.6278\n",
      "Iteration: 3710; Percent complete: 61.8%; Average loss: 2.6231\n",
      "Iteration: 3720; Percent complete: 62.0%; Average loss: 2.6088\n",
      "Iteration: 3730; Percent complete: 62.2%; Average loss: 2.6242\n",
      "Iteration: 3740; Percent complete: 62.3%; Average loss: 2.6104\n",
      "Iteration: 3750; Percent complete: 62.5%; Average loss: 2.6289\n",
      "Iteration: 3760; Percent complete: 62.7%; Average loss: 2.5998\n",
      "Iteration: 3770; Percent complete: 62.8%; Average loss: 2.5948\n",
      "Iteration: 3780; Percent complete: 63.0%; Average loss: 2.5689\n",
      "Iteration: 3790; Percent complete: 63.2%; Average loss: 2.5950\n",
      "Iteration: 3800; Percent complete: 63.3%; Average loss: 2.6031\n",
      "Iteration: 3810; Percent complete: 63.5%; Average loss: 2.5700\n",
      "Iteration: 3820; Percent complete: 63.7%; Average loss: 2.5806\n",
      "Iteration: 3830; Percent complete: 63.8%; Average loss: 2.6134\n",
      "Iteration: 3840; Percent complete: 64.0%; Average loss: 2.6083\n",
      "Iteration: 3850; Percent complete: 64.2%; Average loss: 2.5736\n",
      "Iteration: 3860; Percent complete: 64.3%; Average loss: 2.5815\n",
      "Iteration: 3870; Percent complete: 64.5%; Average loss: 2.5557\n",
      "Iteration: 3880; Percent complete: 64.7%; Average loss: 2.5793\n",
      "Iteration: 3890; Percent complete: 64.8%; Average loss: 2.5505\n",
      "Iteration: 3900; Percent complete: 65.0%; Average loss: 2.5531\n",
      "Iteration: 3910; Percent complete: 65.2%; Average loss: 2.5603\n",
      "Iteration: 3920; Percent complete: 65.3%; Average loss: 2.5415\n",
      "Iteration: 3930; Percent complete: 65.5%; Average loss: 2.5562\n",
      "Iteration: 3940; Percent complete: 65.7%; Average loss: 2.5532\n",
      "Iteration: 3950; Percent complete: 65.8%; Average loss: 2.4939\n",
      "Iteration: 3960; Percent complete: 66.0%; Average loss: 2.5077\n",
      "Iteration: 3970; Percent complete: 66.2%; Average loss: 2.5237\n",
      "Iteration: 3980; Percent complete: 66.3%; Average loss: 2.5452\n",
      "Iteration: 3990; Percent complete: 66.5%; Average loss: 2.5337\n",
      "Iteration: 4000; Percent complete: 66.7%; Average loss: 2.5329\n",
      "./content/cb_model/Chat/2-4_512\n",
      "Iteration: 4010; Percent complete: 66.8%; Average loss: 2.5353\n",
      "Iteration: 4020; Percent complete: 67.0%; Average loss: 2.5007\n",
      "Iteration: 4030; Percent complete: 67.2%; Average loss: 2.5017\n",
      "Iteration: 4040; Percent complete: 67.3%; Average loss: 2.5052\n",
      "Iteration: 4050; Percent complete: 67.5%; Average loss: 2.5342\n",
      "Iteration: 4060; Percent complete: 67.7%; Average loss: 2.4854\n",
      "Iteration: 4070; Percent complete: 67.8%; Average loss: 2.4890\n",
      "Iteration: 4080; Percent complete: 68.0%; Average loss: 2.4651\n",
      "Iteration: 4090; Percent complete: 68.2%; Average loss: 2.5115\n",
      "Iteration: 4100; Percent complete: 68.3%; Average loss: 2.4929\n",
      "Iteration: 4110; Percent complete: 68.5%; Average loss: 2.4500\n",
      "Iteration: 4120; Percent complete: 68.7%; Average loss: 2.4611\n",
      "Iteration: 4130; Percent complete: 68.8%; Average loss: 2.4906\n",
      "Iteration: 4140; Percent complete: 69.0%; Average loss: 2.4981\n",
      "Iteration: 4150; Percent complete: 69.2%; Average loss: 2.4719\n",
      "Iteration: 4160; Percent complete: 69.3%; Average loss: 2.4619\n",
      "Iteration: 4170; Percent complete: 69.5%; Average loss: 2.4549\n",
      "Iteration: 4180; Percent complete: 69.7%; Average loss: 2.4725\n",
      "Iteration: 4190; Percent complete: 69.8%; Average loss: 2.4554\n",
      "Iteration: 4200; Percent complete: 70.0%; Average loss: 2.4434\n",
      "Iteration: 4210; Percent complete: 70.2%; Average loss: 2.4428\n",
      "Iteration: 4220; Percent complete: 70.3%; Average loss: 2.4394\n",
      "Iteration: 4230; Percent complete: 70.5%; Average loss: 2.4237\n",
      "Iteration: 4240; Percent complete: 70.7%; Average loss: 2.4356\n",
      "Iteration: 4250; Percent complete: 70.8%; Average loss: 2.4330\n",
      "Iteration: 4260; Percent complete: 71.0%; Average loss: 2.4263\n",
      "Iteration: 4270; Percent complete: 71.2%; Average loss: 2.4736\n",
      "Iteration: 4280; Percent complete: 71.3%; Average loss: 2.3922\n",
      "Iteration: 4290; Percent complete: 71.5%; Average loss: 2.4403\n",
      "Iteration: 4300; Percent complete: 71.7%; Average loss: 2.4206\n",
      "Iteration: 4310; Percent complete: 71.8%; Average loss: 2.3911\n",
      "Iteration: 4320; Percent complete: 72.0%; Average loss: 2.4304\n",
      "Iteration: 4330; Percent complete: 72.2%; Average loss: 2.4238\n",
      "Iteration: 4340; Percent complete: 72.3%; Average loss: 2.3859\n",
      "Iteration: 4350; Percent complete: 72.5%; Average loss: 2.4014\n",
      "Iteration: 4360; Percent complete: 72.7%; Average loss: 2.3801\n",
      "Iteration: 4370; Percent complete: 72.8%; Average loss: 2.3889\n",
      "Iteration: 4380; Percent complete: 73.0%; Average loss: 2.3954\n",
      "Iteration: 4390; Percent complete: 73.2%; Average loss: 2.3934\n",
      "Iteration: 4400; Percent complete: 73.3%; Average loss: 2.3880\n",
      "Iteration: 4410; Percent complete: 73.5%; Average loss: 2.3639\n",
      "Iteration: 4420; Percent complete: 73.7%; Average loss: 2.4041\n",
      "Iteration: 4430; Percent complete: 73.8%; Average loss: 2.3752\n",
      "Iteration: 4440; Percent complete: 74.0%; Average loss: 2.3550\n",
      "Iteration: 4450; Percent complete: 74.2%; Average loss: 2.3007\n",
      "Iteration: 4460; Percent complete: 74.3%; Average loss: 2.3583\n",
      "Iteration: 4470; Percent complete: 74.5%; Average loss: 2.3395\n",
      "Iteration: 4480; Percent complete: 74.7%; Average loss: 2.3784\n",
      "Iteration: 4490; Percent complete: 74.8%; Average loss: 2.3457\n",
      "Iteration: 4500; Percent complete: 75.0%; Average loss: 2.3672\n",
      "Iteration: 4510; Percent complete: 75.2%; Average loss: 2.3501\n",
      "Iteration: 4520; Percent complete: 75.3%; Average loss: 2.3428\n",
      "Iteration: 4530; Percent complete: 75.5%; Average loss: 2.3175\n",
      "Iteration: 4540; Percent complete: 75.7%; Average loss: 2.3622\n",
      "Iteration: 4550; Percent complete: 75.8%; Average loss: 2.3398\n",
      "Iteration: 4560; Percent complete: 76.0%; Average loss: 2.3292\n",
      "Iteration: 4570; Percent complete: 76.2%; Average loss: 2.2987\n",
      "Iteration: 4580; Percent complete: 76.3%; Average loss: 2.3592\n",
      "Iteration: 4590; Percent complete: 76.5%; Average loss: 2.3338\n",
      "Iteration: 4600; Percent complete: 76.7%; Average loss: 2.3360\n",
      "Iteration: 4610; Percent complete: 76.8%; Average loss: 2.3264\n",
      "Iteration: 4620; Percent complete: 77.0%; Average loss: 2.3359\n",
      "Iteration: 4630; Percent complete: 77.2%; Average loss: 2.2968\n",
      "Iteration: 4640; Percent complete: 77.3%; Average loss: 2.3132\n",
      "Iteration: 4650; Percent complete: 77.5%; Average loss: 2.3000\n",
      "Iteration: 4660; Percent complete: 77.7%; Average loss: 2.3108\n",
      "Iteration: 4670; Percent complete: 77.8%; Average loss: 2.2727\n",
      "Iteration: 4680; Percent complete: 78.0%; Average loss: 2.2553\n",
      "Iteration: 4690; Percent complete: 78.2%; Average loss: 2.3184\n",
      "Iteration: 4700; Percent complete: 78.3%; Average loss: 2.2921\n",
      "Iteration: 4710; Percent complete: 78.5%; Average loss: 2.2876\n",
      "Iteration: 4720; Percent complete: 78.7%; Average loss: 2.2959\n",
      "Iteration: 4730; Percent complete: 78.8%; Average loss: 2.2667\n",
      "Iteration: 4740; Percent complete: 79.0%; Average loss: 2.2736\n",
      "Iteration: 4750; Percent complete: 79.2%; Average loss: 2.2741\n",
      "Iteration: 4760; Percent complete: 79.3%; Average loss: 2.2708\n",
      "Iteration: 4770; Percent complete: 79.5%; Average loss: 2.2819\n",
      "Iteration: 4780; Percent complete: 79.7%; Average loss: 2.2679\n",
      "Iteration: 4790; Percent complete: 79.8%; Average loss: 2.2467\n",
      "Iteration: 4800; Percent complete: 80.0%; Average loss: 2.2857\n",
      "Iteration: 4810; Percent complete: 80.2%; Average loss: 2.2446\n",
      "Iteration: 4820; Percent complete: 80.3%; Average loss: 2.2471\n",
      "Iteration: 4830; Percent complete: 80.5%; Average loss: 2.2753\n",
      "Iteration: 4840; Percent complete: 80.7%; Average loss: 2.2399\n",
      "Iteration: 4850; Percent complete: 80.8%; Average loss: 2.2397\n",
      "Iteration: 4860; Percent complete: 81.0%; Average loss: 2.2350\n",
      "Iteration: 4870; Percent complete: 81.2%; Average loss: 2.2058\n",
      "Iteration: 4880; Percent complete: 81.3%; Average loss: 2.2384\n",
      "Iteration: 4890; Percent complete: 81.5%; Average loss: 2.2533\n",
      "Iteration: 4900; Percent complete: 81.7%; Average loss: 2.1869\n",
      "Iteration: 4910; Percent complete: 81.8%; Average loss: 2.1937\n",
      "Iteration: 4920; Percent complete: 82.0%; Average loss: 2.2367\n",
      "Iteration: 4930; Percent complete: 82.2%; Average loss: 2.2015\n",
      "Iteration: 4940; Percent complete: 82.3%; Average loss: 2.2532\n",
      "Iteration: 4950; Percent complete: 82.5%; Average loss: 2.2241\n",
      "Iteration: 4960; Percent complete: 82.7%; Average loss: 2.2096\n",
      "Iteration: 4970; Percent complete: 82.8%; Average loss: 2.2102\n",
      "Iteration: 4980; Percent complete: 83.0%; Average loss: 2.2104\n",
      "Iteration: 4990; Percent complete: 83.2%; Average loss: 2.1596\n",
      "Iteration: 5000; Percent complete: 83.3%; Average loss: 2.1805\n",
      "Iteration: 5010; Percent complete: 83.5%; Average loss: 2.1768\n",
      "Iteration: 5020; Percent complete: 83.7%; Average loss: 2.1980\n",
      "Iteration: 5030; Percent complete: 83.8%; Average loss: 2.1534\n",
      "Iteration: 5040; Percent complete: 84.0%; Average loss: 2.1443\n",
      "Iteration: 5050; Percent complete: 84.2%; Average loss: 2.1489\n",
      "Iteration: 5060; Percent complete: 84.3%; Average loss: 2.1874\n",
      "Iteration: 5070; Percent complete: 84.5%; Average loss: 2.1631\n",
      "Iteration: 5080; Percent complete: 84.7%; Average loss: 2.1701\n",
      "Iteration: 5090; Percent complete: 84.8%; Average loss: 2.1756\n",
      "Iteration: 5100; Percent complete: 85.0%; Average loss: 2.1686\n",
      "Iteration: 5110; Percent complete: 85.2%; Average loss: 2.1468\n",
      "Iteration: 5120; Percent complete: 85.3%; Average loss: 2.1075\n",
      "Iteration: 5130; Percent complete: 85.5%; Average loss: 2.1433\n",
      "Iteration: 5140; Percent complete: 85.7%; Average loss: 2.1571\n",
      "Iteration: 5150; Percent complete: 85.8%; Average loss: 2.1243\n",
      "Iteration: 5160; Percent complete: 86.0%; Average loss: 2.1207\n",
      "Iteration: 5170; Percent complete: 86.2%; Average loss: 2.1248\n",
      "Iteration: 5180; Percent complete: 86.3%; Average loss: 2.1307\n",
      "Iteration: 5190; Percent complete: 86.5%; Average loss: 2.1170\n",
      "Iteration: 5200; Percent complete: 86.7%; Average loss: 2.1780\n",
      "Iteration: 5210; Percent complete: 86.8%; Average loss: 2.1300\n",
      "Iteration: 5220; Percent complete: 87.0%; Average loss: 2.1249\n",
      "Iteration: 5230; Percent complete: 87.2%; Average loss: 2.1355\n",
      "Iteration: 5240; Percent complete: 87.3%; Average loss: 2.1572\n",
      "Iteration: 5250; Percent complete: 87.5%; Average loss: 2.0880\n",
      "Iteration: 5260; Percent complete: 87.7%; Average loss: 2.1092\n",
      "Iteration: 5270; Percent complete: 87.8%; Average loss: 2.0829\n",
      "Iteration: 5280; Percent complete: 88.0%; Average loss: 2.0936\n",
      "Iteration: 5290; Percent complete: 88.2%; Average loss: 2.0897\n",
      "Iteration: 5300; Percent complete: 88.3%; Average loss: 2.1102\n",
      "Iteration: 5310; Percent complete: 88.5%; Average loss: 2.1052\n",
      "Iteration: 5320; Percent complete: 88.7%; Average loss: 2.1170\n",
      "Iteration: 5330; Percent complete: 88.8%; Average loss: 2.0579\n",
      "Iteration: 5340; Percent complete: 89.0%; Average loss: 2.0625\n",
      "Iteration: 5350; Percent complete: 89.2%; Average loss: 2.0761\n",
      "Iteration: 5360; Percent complete: 89.3%; Average loss: 2.0963\n",
      "Iteration: 5370; Percent complete: 89.5%; Average loss: 2.0716\n",
      "Iteration: 5380; Percent complete: 89.7%; Average loss: 2.0928\n",
      "Iteration: 5390; Percent complete: 89.8%; Average loss: 2.0617\n",
      "Iteration: 5400; Percent complete: 90.0%; Average loss: 2.0755\n",
      "Iteration: 5410; Percent complete: 90.2%; Average loss: 2.0669\n",
      "Iteration: 5420; Percent complete: 90.3%; Average loss: 2.0743\n",
      "Iteration: 5430; Percent complete: 90.5%; Average loss: 2.0964\n",
      "Iteration: 5440; Percent complete: 90.7%; Average loss: 2.0703\n",
      "Iteration: 5450; Percent complete: 90.8%; Average loss: 2.0712\n",
      "Iteration: 5460; Percent complete: 91.0%; Average loss: 2.0487\n",
      "Iteration: 5470; Percent complete: 91.2%; Average loss: 2.0468\n",
      "Iteration: 5480; Percent complete: 91.3%; Average loss: 2.0352\n",
      "Iteration: 5490; Percent complete: 91.5%; Average loss: 2.0433\n",
      "Iteration: 5500; Percent complete: 91.7%; Average loss: 2.0581\n",
      "Iteration: 5510; Percent complete: 91.8%; Average loss: 2.0108\n",
      "Iteration: 5520; Percent complete: 92.0%; Average loss: 2.0243\n",
      "Iteration: 5530; Percent complete: 92.2%; Average loss: 2.0449\n",
      "Iteration: 5540; Percent complete: 92.3%; Average loss: 2.0318\n",
      "Iteration: 5550; Percent complete: 92.5%; Average loss: 2.0194\n",
      "Iteration: 5560; Percent complete: 92.7%; Average loss: 2.0242\n",
      "Iteration: 5570; Percent complete: 92.8%; Average loss: 2.0243\n",
      "Iteration: 5580; Percent complete: 93.0%; Average loss: 2.0158\n",
      "Iteration: 5590; Percent complete: 93.2%; Average loss: 2.0362\n",
      "Iteration: 5600; Percent complete: 93.3%; Average loss: 2.0066\n",
      "Iteration: 5610; Percent complete: 93.5%; Average loss: 1.9977\n",
      "Iteration: 5620; Percent complete: 93.7%; Average loss: 2.0045\n",
      "Iteration: 5630; Percent complete: 93.8%; Average loss: 1.9992\n",
      "Iteration: 5640; Percent complete: 94.0%; Average loss: 2.0209\n",
      "Iteration: 5650; Percent complete: 94.2%; Average loss: 2.0009\n",
      "Iteration: 5660; Percent complete: 94.3%; Average loss: 1.9769\n",
      "Iteration: 5670; Percent complete: 94.5%; Average loss: 1.9743\n",
      "Iteration: 5680; Percent complete: 94.7%; Average loss: 1.9632\n",
      "Iteration: 5690; Percent complete: 94.8%; Average loss: 1.9878\n",
      "Iteration: 5700; Percent complete: 95.0%; Average loss: 1.9606\n",
      "Iteration: 5710; Percent complete: 95.2%; Average loss: 1.9538\n",
      "Iteration: 5720; Percent complete: 95.3%; Average loss: 1.9914\n",
      "Iteration: 5730; Percent complete: 95.5%; Average loss: 1.9395\n",
      "Iteration: 5740; Percent complete: 95.7%; Average loss: 1.9815\n",
      "Iteration: 5750; Percent complete: 95.8%; Average loss: 2.0034\n",
      "Iteration: 5760; Percent complete: 96.0%; Average loss: 1.9756\n",
      "Iteration: 5770; Percent complete: 96.2%; Average loss: 1.9260\n",
      "Iteration: 5780; Percent complete: 96.3%; Average loss: 1.9432\n",
      "Iteration: 5790; Percent complete: 96.5%; Average loss: 1.9451\n",
      "Iteration: 5800; Percent complete: 96.7%; Average loss: 1.9726\n",
      "Iteration: 5810; Percent complete: 96.8%; Average loss: 1.8700\n",
      "Iteration: 5820; Percent complete: 97.0%; Average loss: 1.9320\n",
      "Iteration: 5830; Percent complete: 97.2%; Average loss: 1.9062\n",
      "Iteration: 5840; Percent complete: 97.3%; Average loss: 1.9406\n",
      "Iteration: 5850; Percent complete: 97.5%; Average loss: 1.9322\n",
      "Iteration: 5860; Percent complete: 97.7%; Average loss: 1.9507\n",
      "Iteration: 5870; Percent complete: 97.8%; Average loss: 1.9414\n",
      "Iteration: 5880; Percent complete: 98.0%; Average loss: 1.9237\n",
      "Iteration: 5890; Percent complete: 98.2%; Average loss: 1.9065\n",
      "Iteration: 5900; Percent complete: 98.3%; Average loss: 1.9184\n",
      "Iteration: 5910; Percent complete: 98.5%; Average loss: 1.9216\n",
      "Iteration: 5920; Percent complete: 98.7%; Average loss: 1.9407\n",
      "Iteration: 5930; Percent complete: 98.8%; Average loss: 1.8727\n",
      "Iteration: 5940; Percent complete: 99.0%; Average loss: 1.9000\n",
      "Iteration: 5950; Percent complete: 99.2%; Average loss: 1.9149\n",
      "Iteration: 5960; Percent complete: 99.3%; Average loss: 1.8911\n",
      "Iteration: 5970; Percent complete: 99.5%; Average loss: 1.8857\n",
      "Iteration: 5980; Percent complete: 99.7%; Average loss: 1.8778\n",
      "Iteration: 5990; Percent complete: 99.8%; Average loss: 1.9062\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 1.8761\n",
      "./content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = './content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 10\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "source": [
    "# Plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 362.5625 248.518125\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 362.5625 248.518125 \nL 362.5625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 224.64 \nL 355.3625 224.64 \nL 355.3625 7.2 \nL 20.5625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md375add92f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#md375add92f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(32.599432 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.592641\" xlink:href=\"#md375add92f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(77.048891 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"137.4046\" xlink:href=\"#md375add92f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(127.86085 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.21656\" xlink:href=\"#md375add92f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(178.67281 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.028519\" xlink:href=\"#md375add92f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(229.484769 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.840478\" xlink:href=\"#md375add92f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(280.296728 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"340.652438\" xlink:href=\"#md375add92f\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(331.108688 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me2f2687e3d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me2f2687e3d\" y=\"210.716949\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 214.516168)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me2f2687e3d\" y=\"179.637313\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 183.436532)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me2f2687e3d\" y=\"148.557678\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 152.356896)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me2f2687e3d\" y=\"117.478042\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(7.2 121.277261)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me2f2687e3d\" y=\"86.398406\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 6 -->\n      <g transform=\"translate(7.2 90.197625)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me2f2687e3d\" y=\"55.318771\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 59.117989)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me2f2687e3d\" y=\"24.239135\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(7.2 28.038354)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p535d743a8b)\" d=\"M 35.780682 17.083636 \nL 36.288801 93.15813 \nL 36.796921 114.670285 \nL 37.305041 116.068164 \nL 37.81316 118.101653 \nL 38.32128 122.428422 \nL 38.829399 121.939654 \nL 39.845639 124.357412 \nL 40.353758 125.122197 \nL 40.861878 124.814413 \nL 41.369997 124.841039 \nL 41.878117 125.919608 \nL 42.386237 125.803033 \nL 42.894356 127.50086 \nL 43.402476 126.02459 \nL 43.910595 126.555788 \nL 44.418715 128.012906 \nL 44.926834 127.90025 \nL 45.434954 128.313751 \nL 45.943074 127.263227 \nL 46.451193 128.460597 \nL 46.959313 129.007966 \nL 47.975552 135.906948 \nL 48.483672 136.058351 \nL 48.991791 135.801172 \nL 49.499911 135.826658 \nL 50.51615 136.143737 \nL 51.02427 137.280195 \nL 51.532389 137.866744 \nL 52.040509 137.737277 \nL 52.548628 137.180919 \nL 53.056748 138.293908 \nL 53.564868 137.699783 \nL 54.072987 137.498895 \nL 54.581107 138.139945 \nL 55.089226 137.799027 \nL 55.597346 137.662656 \nL 56.105466 139.555316 \nL 56.613585 139.067551 \nL 57.121705 140.650892 \nL 57.629824 139.639672 \nL 58.137944 140.559601 \nL 59.154183 141.397878 \nL 59.662303 142.509953 \nL 60.170422 142.123433 \nL 60.678542 143.943484 \nL 61.186661 142.811894 \nL 61.694781 144.150611 \nL 62.202901 144.706751 \nL 62.71102 144.07506 \nL 63.21914 144.755968 \nL 63.727259 144.447724 \nL 64.235379 144.560332 \nL 64.743499 145.526661 \nL 65.251618 146.247049 \nL 65.759738 146.572234 \nL 66.267857 146.502022 \nL 66.775977 145.69562 \nL 67.284097 147.745677 \nL 67.792216 147.479209 \nL 68.300336 148.451901 \nL 68.808455 148.450951 \nL 69.316575 147.260362 \nL 69.824695 148.156406 \nL 70.332814 147.869702 \nL 70.840934 148.599139 \nL 71.349053 148.143407 \nL 72.365293 150.471149 \nL 72.873412 148.495824 \nL 73.381532 150.626439 \nL 73.889651 150.621882 \nL 74.90589 148.593809 \nL 75.41401 149.863547 \nL 75.92213 149.608062 \nL 76.938369 150.172912 \nL 77.446488 150.276322 \nL 77.954608 150.082949 \nL 78.462728 151.965643 \nL 78.970847 151.048208 \nL 79.478967 152.856198 \nL 79.987086 152.832855 \nL 80.495206 152.429486 \nL 81.003326 151.831039 \nL 81.511445 152.704473 \nL 82.019565 153.857549 \nL 82.527684 153.068802 \nL 83.035804 154.284171 \nL 83.543924 153.997466 \nL 84.560163 153.938179 \nL 85.068282 154.31692 \nL 85.576402 153.91761 \nL 86.084522 153.803627 \nL 86.592641 153.997365 \nL 87.100761 155.7481 \nL 87.60888 155.102308 \nL 88.117 156.331369 \nL 89.133239 154.729107 \nL 89.641359 155.753527 \nL 90.149478 156.462797 \nL 90.657598 156.37416 \nL 91.165717 154.698425 \nL 91.673837 157.001886 \nL 92.181957 155.512521 \nL 92.690076 157.11809 \nL 93.198196 156.527746 \nL 93.706315 157.386525 \nL 94.214435 157.175808 \nL 94.722555 157.84345 \nL 95.230674 157.910561 \nL 95.738794 157.504796 \nL 96.246913 157.582542 \nL 96.755033 158.670976 \nL 97.263153 158.238365 \nL 97.771272 158.496273 \nL 98.787511 158.417458 \nL 99.803751 159.208407 \nL 100.31187 160.18183 \nL 100.81999 159.444488 \nL 101.328109 159.216373 \nL 101.836229 159.581271 \nL 102.344349 159.106182 \nL 102.852468 160.344933 \nL 103.360588 159.340559 \nL 103.868707 159.721031 \nL 104.376827 160.590765 \nL 104.884947 160.210185 \nL 105.393066 161.147329 \nL 105.901186 161.550701 \nL 106.409305 161.520353 \nL 106.917425 160.733737 \nL 107.425544 162.577789 \nL 107.933664 162.501532 \nL 108.441784 162.269111 \nL 108.949903 162.496667 \nL 109.458023 162.121211 \nL 110.474262 162.519393 \nL 110.982382 161.225149 \nL 111.490501 163.67462 \nL 111.998621 163.086973 \nL 112.50674 163.699325 \nL 113.01486 163.070981 \nL 113.52298 163.628779 \nL 114.031099 164.4503 \nL 114.539219 163.736101 \nL 115.047338 165.115554 \nL 115.555458 165.227964 \nL 116.571697 164.095836 \nL 117.079817 165.139906 \nL 117.587936 165.493176 \nL 118.096056 165.468348 \nL 119.112295 165.249535 \nL 119.620415 165.390313 \nL 120.128534 164.386377 \nL 120.636654 165.133622 \nL 121.144773 167.603766 \nL 121.652893 166.668926 \nL 122.161013 167.346747 \nL 122.669132 166.048285 \nL 123.177252 166.267915 \nL 123.685371 167.716465 \nL 124.193491 166.989062 \nL 125.20973 167.081078 \nL 125.71785 167.261622 \nL 126.225969 167.002224 \nL 126.734089 168.603106 \nL 127.242209 167.833157 \nL 127.750328 167.4194 \nL 128.766567 169.24444 \nL 129.274687 167.699385 \nL 129.782807 169.394759 \nL 130.290926 168.886411 \nL 130.799046 167.472836 \nL 131.307165 168.301718 \nL 131.815285 169.487519 \nL 132.323405 169.14471 \nL 132.831524 170.097902 \nL 133.339644 170.34799 \nL 133.847763 169.711225 \nL 134.355883 170.869491 \nL 134.864003 169.87216 \nL 135.880242 169.641978 \nL 136.388361 170.007561 \nL 136.896481 169.762916 \nL 137.4046 171.318805 \nL 137.91272 170.175725 \nL 138.42084 171.241299 \nL 139.437079 171.93513 \nL 140.453318 170.834393 \nL 140.961438 170.813791 \nL 141.469557 172.723311 \nL 141.977677 171.882222 \nL 142.485796 172.193284 \nL 142.993916 171.229221 \nL 143.502036 173.20268 \nL 144.010155 172.649614 \nL 144.518275 172.651364 \nL 145.026394 171.788804 \nL 145.534514 173.430447 \nL 146.042634 172.81689 \nL 146.550753 173.463566 \nL 147.058873 173.688168 \nL 147.566992 173.233892 \nL 148.075112 172.624371 \nL 149.599471 175.043264 \nL 150.10759 173.513118 \nL 151.123829 175.512338 \nL 151.631949 174.823087 \nL 152.140069 175.121227 \nL 152.648188 174.585339 \nL 153.156308 174.446188 \nL 153.664427 174.027441 \nL 154.172547 175.083749 \nL 154.680667 175.209932 \nL 155.188786 175.631678 \nL 155.696906 177.011391 \nL 156.205025 177.008925 \nL 156.713145 175.702389 \nL 157.221265 176.508049 \nL 157.729384 176.33674 \nL 158.237504 176.304867 \nL 158.745623 176.113505 \nL 159.253743 176.134212 \nL 159.761863 175.885257 \nL 160.269982 176.393482 \nL 160.778102 177.477986 \nL 161.286221 176.786012 \nL 161.794341 175.801345 \nL 162.302461 175.660997 \nL 162.81058 176.110516 \nL 163.3187 176.850709 \nL 163.826819 176.848813 \nL 164.334939 177.386849 \nL 164.843059 177.161284 \nL 165.351178 178.26151 \nL 165.859298 177.801248 \nL 166.367417 178.280444 \nL 166.875537 179.154586 \nL 167.891776 178.224368 \nL 168.399896 177.451123 \nL 168.908015 179.530713 \nL 169.416135 178.900167 \nL 169.924254 179.981182 \nL 170.432374 178.723727 \nL 170.940494 179.989916 \nL 171.448613 179.146295 \nL 172.464852 180.121776 \nL 172.972972 179.776375 \nL 173.481092 180.446396 \nL 173.989211 180.64281 \nL 174.497331 179.715522 \nL 175.00545 180.410656 \nL 175.51357 180.233931 \nL 176.02169 179.883146 \nL 176.529809 179.982724 \nL 177.037929 181.539886 \nL 177.546048 180.504176 \nL 178.054168 180.463593 \nL 178.562288 181.271547 \nL 179.070407 180.939238 \nL 180.086646 181.829263 \nL 180.594766 180.850561 \nL 181.102885 181.288558 \nL 181.611005 182.488201 \nL 182.119125 183.204409 \nL 182.627244 181.630313 \nL 183.135364 182.509938 \nL 183.643483 182.741146 \nL 184.151603 183.95821 \nL 184.659723 182.487064 \nL 185.167842 182.559132 \nL 185.675962 181.494062 \nL 186.184081 182.412026 \nL 186.692201 182.356264 \nL 187.200321 184.39591 \nL 187.70844 184.176698 \nL 188.21656 183.18748 \nL 188.724679 182.721774 \nL 189.232799 183.358907 \nL 189.740919 183.260505 \nL 190.757158 183.634087 \nL 191.265277 184.240859 \nL 191.773397 183.695695 \nL 192.281517 183.441993 \nL 193.297756 184.192116 \nL 193.805875 184.262983 \nL 194.313995 185.909047 \nL 194.822115 184.795327 \nL 195.330234 185.303237 \nL 195.838354 184.754076 \nL 196.346473 185.767967 \nL 196.854593 186.251389 \nL 197.362712 184.894365 \nL 197.870832 185.394795 \nL 198.378952 184.933987 \nL 198.887071 185.89077 \nL 199.90331 186.64841 \nL 200.41143 186.364123 \nL 200.91955 187.224224 \nL 201.427669 187.097074 \nL 201.935789 185.858984 \nL 202.443908 187.481388 \nL 202.952028 187.147229 \nL 203.460148 187.208768 \nL 203.968267 187.403972 \nL 204.476387 186.963135 \nL 204.984506 187.020976 \nL 206.000746 187.378502 \nL 206.508865 186.347661 \nL 207.016985 187.346719 \nL 207.525104 187.63019 \nL 208.033224 188.973352 \nL 208.541344 188.347472 \nL 209.049463 188.281361 \nL 209.557583 187.549091 \nL 210.065702 188.754234 \nL 211.590061 188.709949 \nL 212.098181 188.477965 \nL 212.6063 189.110369 \nL 213.11442 188.484958 \nL 213.622539 188.371298 \nL 214.130659 189.669248 \nL 214.638779 189.267434 \nL 215.146898 189.712333 \nL 215.655018 189.923025 \nL 216.163137 189.987463 \nL 216.671257 190.259058 \nL 217.179377 189.557941 \nL 218.703735 189.582464 \nL 219.211855 190.659556 \nL 220.228094 190.15571 \nL 220.736214 190.866833 \nL 221.244333 191.026486 \nL 221.752453 189.829027 \nL 222.260573 190.795822 \nL 222.768692 191.117017 \nL 223.784931 191.350477 \nL 224.293051 191.794624 \nL 224.801171 191.315751 \nL 225.30929 191.745962 \nL 225.81741 191.171927 \nL 226.325529 192.074578 \nL 226.833649 192.232073 \nL 227.341768 193.03698 \nL 227.849888 192.224117 \nL 228.358008 191.972212 \nL 228.866127 193.000397 \nL 229.374247 192.672972 \nL 229.882366 191.651417 \nL 230.390486 191.812454 \nL 230.898606 192.889639 \nL 231.406725 192.645139 \nL 231.914845 193.446487 \nL 232.422964 192.711267 \nL 232.931084 193.606533 \nL 233.439204 193.528179 \nL 233.947323 193.302825 \nL 234.455443 193.88708 \nL 234.963562 193.4311 \nL 235.471682 193.523222 \nL 235.979802 195.367794 \nL 236.996041 194.439215 \nL 237.50416 193.771751 \nL 238.01228 194.128801 \nL 239.028519 194.078561 \nL 239.536639 195.154938 \nL 240.552878 195.015961 \nL 241.060997 194.113421 \nL 241.569117 195.629997 \nL 242.077237 195.51904 \nL 242.585356 196.261614 \nL 243.093476 194.820629 \nL 243.601595 195.39713 \nL 244.109715 196.730218 \nL 244.617835 196.38702 \nL 245.125954 195.468524 \nL 245.634074 195.237028 \nL 246.142193 196.049968 \nL 247.158433 196.579314 \nL 247.666552 196.032833 \nL 248.682791 196.937519 \nL 249.699031 197.061024 \nL 250.20715 197.548313 \nL 250.71527 197.179181 \nL 251.223389 197.258916 \nL 251.731509 197.467511 \nL 252.239629 195.996393 \nL 252.747748 198.527665 \nL 253.255868 197.033191 \nL 253.763987 197.643587 \nL 254.272107 198.56167 \nL 254.780227 197.340531 \nL 255.288346 197.54637 \nL 255.796466 198.721935 \nL 256.304585 198.240458 \nL 256.812705 198.902097 \nL 257.828944 198.426514 \nL 258.845183 198.659108 \nL 259.353303 199.406331 \nL 259.861422 198.157191 \nL 260.369542 199.057103 \nL 260.877662 199.68369 \nL 261.385781 201.371061 \nL 261.893901 199.58047 \nL 262.40202 200.166364 \nL 262.91014 198.954878 \nL 263.41826 199.971303 \nL 263.926379 199.304091 \nL 264.434499 199.834814 \nL 264.942618 200.063592 \nL 265.450738 200.849543 \nL 265.958858 199.458528 \nL 266.466977 200.154741 \nL 266.975097 200.486035 \nL 267.483216 201.434528 \nL 267.991336 199.552263 \nL 268.499456 200.342773 \nL 269.007575 200.272832 \nL 269.515695 200.572704 \nL 270.023814 200.27856 \nL 270.531934 201.491814 \nL 271.040053 200.983675 \nL 271.548173 201.394053 \nL 272.056293 201.057197 \nL 272.564412 202.242143 \nL 273.072532 202.780837 \nL 273.580651 200.820162 \nL 274.088771 201.638143 \nL 274.596891 201.778939 \nL 275.10501 201.519988 \nL 275.61313 202.429033 \nL 276.121249 202.215058 \nL 276.629369 202.199343 \nL 277.137489 202.300742 \nL 277.645608 201.955414 \nL 278.153728 202.391134 \nL 278.661847 203.050429 \nL 279.169967 201.838291 \nL 279.678087 203.115964 \nL 280.186206 203.037486 \nL 280.694326 202.159392 \nL 281.202445 203.261925 \nL 281.710565 203.266148 \nL 282.218685 203.413212 \nL 282.726804 204.322221 \nL 283.234924 203.307319 \nL 283.743043 202.845327 \nL 284.251163 204.90913 \nL 284.759283 204.697871 \nL 285.267402 203.359826 \nL 285.775522 204.453887 \nL 286.283641 202.848035 \nL 286.791761 203.751229 \nL 287.29988 204.202364 \nL 288.31612 204.178448 \nL 288.824239 205.756346 \nL 289.332359 205.108454 \nL 289.840478 205.22179 \nL 290.348598 204.563078 \nL 290.856718 205.95052 \nL 291.364837 206.23149 \nL 291.872957 206.089646 \nL 292.381076 204.89402 \nL 292.889196 205.646614 \nL 293.905435 205.259421 \nL 294.413555 205.477406 \nL 294.921674 206.155929 \nL 295.429794 207.37613 \nL 295.937914 206.261961 \nL 296.446033 205.833385 \nL 296.954153 206.853977 \nL 297.462272 206.964089 \nL 298.478512 206.654333 \nL 298.986631 207.081873 \nL 299.494751 205.184592 \nL 300.00287 206.677262 \nL 300.51099 206.834958 \nL 301.01911 206.505104 \nL 301.527229 205.830137 \nL 302.035349 207.983326 \nL 302.543468 207.32331 \nL 303.051588 208.139765 \nL 303.559707 207.808286 \nL 304.067827 207.928216 \nL 304.575947 207.293299 \nL 305.084066 207.446381 \nL 305.592186 207.081848 \nL 306.100305 208.916263 \nL 306.608425 208.774858 \nL 307.116545 208.351211 \nL 307.624664 207.723701 \nL 308.132784 208.491683 \nL 308.640903 207.833892 \nL 309.149023 208.798499 \nL 309.657143 208.371971 \nL 310.165262 208.638849 \nL 310.673382 208.407967 \nL 311.181501 207.719369 \nL 311.689621 208.530884 \nL 312.197741 208.503999 \nL 312.70586 209.203278 \nL 313.21398 209.263373 \nL 313.722099 209.62403 \nL 314.230219 209.37234 \nL 314.738339 208.912606 \nL 315.246458 210.381997 \nL 315.754578 209.962913 \nL 316.262697 209.321338 \nL 317.278936 210.112761 \nL 317.787056 209.964339 \nL 318.295176 209.962214 \nL 318.803295 210.226177 \nL 319.311415 209.593219 \nL 319.819534 210.51221 \nL 320.327654 210.789727 \nL 320.835774 210.578269 \nL 321.343893 210.740359 \nL 321.852013 210.066977 \nL 322.868252 211.435525 \nL 323.376372 211.514679 \nL 323.884491 211.861026 \nL 324.392611 211.094855 \nL 324.90073 211.942324 \nL 325.40885 212.15258 \nL 325.91697 210.98494 \nL 326.425089 212.596759 \nL 326.933209 211.291957 \nL 327.441328 210.612804 \nL 327.949448 211.475349 \nL 328.457568 213.015894 \nL 328.965687 212.481004 \nL 329.473807 212.42262 \nL 329.981926 211.569814 \nL 330.490046 214.756364 \nL 330.998166 212.829968 \nL 331.506285 213.63305 \nL 332.014405 212.563855 \nL 332.522524 212.822956 \nL 333.030644 212.248986 \nL 333.538763 212.538286 \nL 334.555003 213.624024 \nL 335.063122 213.254337 \nL 335.571242 213.154378 \nL 336.079361 212.55926 \nL 336.587481 214.6737 \nL 337.095601 213.8248 \nL 337.60372 213.361958 \nL 338.11184 214.101362 \nL 339.128079 214.514715 \nL 339.636199 213.631213 \nL 340.144318 214.567273 \nL 340.144318 214.567273 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 224.64 \nL 20.5625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 355.3625 224.64 \nL 355.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 224.64 \nL 355.3625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 7.2 \nL 355.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p535d743a8b\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Bleu score Calculation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.14285714285714285 0.048795003647426664\n",
      "1000 0.1450264152454387 0.056843484656642684\n",
      "2000 0.14753072255370014 0.06116039769331575\n",
      "3000 0.14895153904353514 0.0609095610787617\n",
      "4000 0.15060010141976002 0.06138316126518687\n",
      "5000 0.15079677440406977 0.061652091739354944\n",
      "6000 0.1485929693658303 0.059869507903352895\n",
      "7000 0.15150984654918204 0.0625728436777642\n",
      "8000 0.15100103767838402 0.06235496894946322\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.15056946249175485\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.06199239200578142\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "  \n",
    "  input_sentence = testpairs[i][0]\n",
    "  \n",
    "  reference = testpairs[i][1:]\n",
    "  templist = []\n",
    "  for k in range(len(reference)):\n",
    "    if(reference[k]!=''):\n",
    "      temp = reference[k].split(' ')\n",
    "      templist.append(temp)\n",
    "  \n",
    "  \n",
    "  input_sentence = normalizeString(input_sentence)\n",
    "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "  chencherry = SmoothingFunction()\n",
    "#   print(output_words)\n",
    "#   print(templist)\n",
    "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "  gram1_bleu_score.append(score1)\n",
    "  gram2_bleu_score.append(score2)\n",
    "  if i%1000 == 0:\n",
    "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score) )  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bot: hello . . . singer .\n",
      "Bot: i think you were in a drink\n",
      "Bot: bye . good . up .\n",
      "Bot: hello . . . singer .\n",
      "Bot: get up of the neck . out .\n",
      "Bot: you re a awful . a minute\n",
      "Bot: hi . ! ! !\n",
      "Bot: hi . ! ! !\n",
      "Bot: i think you were a pretty\n",
      "Bot: what is it ? out ! !\n",
      "Bot: i think you were a pretty\n",
      "Bot: hi . ! ! !\n",
      "Bot: hi . ! ! !\n",
      "Bot: hi . ! ! !\n",
      "Bot: bye . good . .\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ed6594a68986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Hi, how are you?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ec18c2cf8348>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)\n",
    "# input\n",
    "# Hi, how are you?\n",
    "# What\n",
    "# I don't understand you\n",
    "# hmm, good bye"
   ]
  },
  {
   "source": [
    "# Beam search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) \n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] \n",
    "            scores = self.sentence_scores[:] \n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        \n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(voc.index2word[self.sentence_idxes[i].item()])\n",
    "       \n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f\"Sentence with indices {self.sentence_idxes} \"\n",
    "        res += f\"and scores {self.sentence_scores}\"\n",
    "        return res\n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "           \n",
    "        \n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "        \n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]\n",
    "\n",
    "\n",
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.voc = voc\n",
    "        self.beam_size = beam_size\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
    "        \n",
    "        \n",
    "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
    "        return all_tokens, None\n",
    "\n",
    "    def __str__(self):\n",
    "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Training!\nInitializing ...\nTraining...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/6000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "026ebc08f42c4e84bf79519b07cfb32a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Average loss: 2.9151\n",
      "Iteration: 2850; Percent complete: 47.5%; Average loss: 2.9108\n",
      "Iteration: 2860; Percent complete: 47.7%; Average loss: 2.9109\n",
      "Iteration: 2870; Percent complete: 47.8%; Average loss: 2.9368\n",
      "Iteration: 2880; Percent complete: 48.0%; Average loss: 2.8995\n",
      "Iteration: 2890; Percent complete: 48.2%; Average loss: 2.8921\n",
      "Iteration: 2900; Percent complete: 48.3%; Average loss: 2.9600\n",
      "Iteration: 2910; Percent complete: 48.5%; Average loss: 2.8657\n",
      "Iteration: 2920; Percent complete: 48.7%; Average loss: 2.9083\n",
      "Iteration: 2930; Percent complete: 48.8%; Average loss: 2.8892\n",
      "Iteration: 2940; Percent complete: 49.0%; Average loss: 2.8665\n",
      "Iteration: 2950; Percent complete: 49.2%; Average loss: 2.8885\n",
      "Iteration: 2960; Percent complete: 49.3%; Average loss: 2.8850\n",
      "Iteration: 2970; Percent complete: 49.5%; Average loss: 2.9011\n",
      "Iteration: 2980; Percent complete: 49.7%; Average loss: 2.9272\n",
      "Iteration: 2990; Percent complete: 49.8%; Average loss: 2.8505\n",
      "Iteration: 3000; Percent complete: 50.0%; Average loss: 2.8698\n",
      "Iteration: 3010; Percent complete: 50.2%; Average loss: 2.8776\n",
      "Iteration: 3020; Percent complete: 50.3%; Average loss: 2.8386\n",
      "Iteration: 3030; Percent complete: 50.5%; Average loss: 2.8714\n",
      "Iteration: 3040; Percent complete: 50.7%; Average loss: 2.8643\n",
      "Iteration: 3050; Percent complete: 50.8%; Average loss: 2.8613\n",
      "Iteration: 3060; Percent complete: 51.0%; Average loss: 2.8612\n",
      "Iteration: 3070; Percent complete: 51.2%; Average loss: 2.8398\n",
      "Iteration: 3080; Percent complete: 51.3%; Average loss: 2.8250\n",
      "Iteration: 3090; Percent complete: 51.5%; Average loss: 2.8719\n",
      "Iteration: 3100; Percent complete: 51.7%; Average loss: 2.8218\n",
      "Iteration: 3110; Percent complete: 51.8%; Average loss: 2.8033\n",
      "Iteration: 3120; Percent complete: 52.0%; Average loss: 2.7958\n",
      "Iteration: 3130; Percent complete: 52.2%; Average loss: 2.7894\n",
      "Iteration: 3140; Percent complete: 52.3%; Average loss: 2.7894\n",
      "Iteration: 3150; Percent complete: 52.5%; Average loss: 2.7919\n",
      "Iteration: 3160; Percent complete: 52.7%; Average loss: 2.8219\n",
      "Iteration: 3170; Percent complete: 52.8%; Average loss: 2.7783\n",
      "Iteration: 3180; Percent complete: 53.0%; Average loss: 2.7888\n",
      "Iteration: 3190; Percent complete: 53.2%; Average loss: 2.7823\n",
      "Iteration: 3200; Percent complete: 53.3%; Average loss: 2.8380\n",
      "Iteration: 3210; Percent complete: 53.5%; Average loss: 2.7703\n",
      "Iteration: 3220; Percent complete: 53.7%; Average loss: 2.7749\n",
      "Iteration: 3230; Percent complete: 53.8%; Average loss: 2.8243\n",
      "Iteration: 3240; Percent complete: 54.0%; Average loss: 2.8180\n",
      "Iteration: 3250; Percent complete: 54.2%; Average loss: 2.7728\n",
      "Iteration: 3260; Percent complete: 54.3%; Average loss: 2.7716\n",
      "Iteration: 3270; Percent complete: 54.5%; Average loss: 2.7330\n",
      "Iteration: 3280; Percent complete: 54.7%; Average loss: 2.7718\n",
      "Iteration: 3290; Percent complete: 54.8%; Average loss: 2.7489\n",
      "Iteration: 3300; Percent complete: 55.0%; Average loss: 2.7319\n",
      "Iteration: 3310; Percent complete: 55.2%; Average loss: 2.7620\n",
      "Iteration: 3320; Percent complete: 55.3%; Average loss: 2.7532\n",
      "Iteration: 3330; Percent complete: 55.5%; Average loss: 2.7494\n",
      "Iteration: 3340; Percent complete: 55.7%; Average loss: 2.7345\n",
      "Iteration: 3350; Percent complete: 55.8%; Average loss: 2.7300\n",
      "Iteration: 3360; Percent complete: 56.0%; Average loss: 2.7036\n",
      "Iteration: 3370; Percent complete: 56.2%; Average loss: 2.7329\n",
      "Iteration: 3380; Percent complete: 56.3%; Average loss: 2.7147\n",
      "Iteration: 3390; Percent complete: 56.5%; Average loss: 2.7043\n",
      "Iteration: 3400; Percent complete: 56.7%; Average loss: 2.7265\n",
      "Iteration: 3410; Percent complete: 56.8%; Average loss: 2.7265\n",
      "Iteration: 3420; Percent complete: 57.0%; Average loss: 2.6576\n",
      "Iteration: 3430; Percent complete: 57.2%; Average loss: 2.6814\n",
      "Iteration: 3440; Percent complete: 57.3%; Average loss: 2.7119\n",
      "Iteration: 3450; Percent complete: 57.5%; Average loss: 2.6895\n",
      "Iteration: 3460; Percent complete: 57.7%; Average loss: 2.6600\n",
      "Iteration: 3470; Percent complete: 57.8%; Average loss: 2.6360\n",
      "Iteration: 3480; Percent complete: 58.0%; Average loss: 2.6756\n",
      "Iteration: 3490; Percent complete: 58.2%; Average loss: 2.6978\n",
      "Iteration: 3500; Percent complete: 58.3%; Average loss: 2.6930\n",
      "Iteration: 3510; Percent complete: 58.5%; Average loss: 2.6596\n",
      "Iteration: 3520; Percent complete: 58.7%; Average loss: 2.6460\n",
      "Iteration: 3530; Percent complete: 58.8%; Average loss: 2.6655\n",
      "Iteration: 3540; Percent complete: 59.0%; Average loss: 2.6757\n",
      "Iteration: 3550; Percent complete: 59.2%; Average loss: 2.6880\n",
      "Iteration: 3560; Percent complete: 59.3%; Average loss: 2.6680\n",
      "Iteration: 3570; Percent complete: 59.5%; Average loss: 2.6446\n",
      "Iteration: 3580; Percent complete: 59.7%; Average loss: 2.6317\n",
      "Iteration: 3590; Percent complete: 59.8%; Average loss: 2.6074\n",
      "Iteration: 3600; Percent complete: 60.0%; Average loss: 2.6220\n",
      "Iteration: 3610; Percent complete: 60.2%; Average loss: 2.6142\n",
      "Iteration: 3620; Percent complete: 60.3%; Average loss: 2.6048\n",
      "Iteration: 3630; Percent complete: 60.5%; Average loss: 2.6341\n",
      "Iteration: 3640; Percent complete: 60.7%; Average loss: 2.6396\n",
      "Iteration: 3650; Percent complete: 60.8%; Average loss: 2.6136\n",
      "Iteration: 3660; Percent complete: 61.0%; Average loss: 2.5841\n",
      "Iteration: 3670; Percent complete: 61.2%; Average loss: 2.6127\n",
      "Iteration: 3680; Percent complete: 61.3%; Average loss: 2.6120\n",
      "Iteration: 3690; Percent complete: 61.5%; Average loss: 2.5756\n",
      "Iteration: 3700; Percent complete: 61.7%; Average loss: 2.6155\n",
      "Iteration: 3710; Percent complete: 61.8%; Average loss: 2.5982\n",
      "Iteration: 3720; Percent complete: 62.0%; Average loss: 2.5446\n",
      "Iteration: 3730; Percent complete: 62.2%; Average loss: 2.5611\n",
      "Iteration: 3740; Percent complete: 62.3%; Average loss: 2.5659\n",
      "Iteration: 3750; Percent complete: 62.5%; Average loss: 2.5674\n",
      "Iteration: 3760; Percent complete: 62.7%; Average loss: 2.5922\n",
      "Iteration: 3770; Percent complete: 62.8%; Average loss: 2.5728\n",
      "Iteration: 3780; Percent complete: 63.0%; Average loss: 2.5622\n",
      "Iteration: 3790; Percent complete: 63.2%; Average loss: 2.5814\n",
      "Iteration: 3800; Percent complete: 63.3%; Average loss: 2.5839\n",
      "Iteration: 3810; Percent complete: 63.5%; Average loss: 2.5703\n",
      "Iteration: 3820; Percent complete: 63.7%; Average loss: 2.5381\n",
      "Iteration: 3830; Percent complete: 63.8%; Average loss: 2.5450\n",
      "Iteration: 3840; Percent complete: 64.0%; Average loss: 2.5438\n",
      "Iteration: 3850; Percent complete: 64.2%; Average loss: 2.5234\n",
      "Iteration: 3860; Percent complete: 64.3%; Average loss: 2.5395\n",
      "Iteration: 3870; Percent complete: 64.5%; Average loss: 2.5410\n",
      "Iteration: 3880; Percent complete: 64.7%; Average loss: 2.4795\n",
      "Iteration: 3890; Percent complete: 64.8%; Average loss: 2.4885\n",
      "Iteration: 3900; Percent complete: 65.0%; Average loss: 2.5094\n",
      "Iteration: 3910; Percent complete: 65.2%; Average loss: 2.4934\n",
      "Iteration: 3920; Percent complete: 65.3%; Average loss: 2.5110\n",
      "Iteration: 3930; Percent complete: 65.5%; Average loss: 2.4643\n",
      "Iteration: 3940; Percent complete: 65.7%; Average loss: 2.5055\n",
      "Iteration: 3950; Percent complete: 65.8%; Average loss: 2.5206\n",
      "Iteration: 3960; Percent complete: 66.0%; Average loss: 2.5125\n",
      "Iteration: 3970; Percent complete: 66.2%; Average loss: 2.4973\n",
      "Iteration: 3980; Percent complete: 66.3%; Average loss: 2.4537\n",
      "Iteration: 3990; Percent complete: 66.5%; Average loss: 2.4682\n",
      "Iteration: 4000; Percent complete: 66.7%; Average loss: 2.4642\n",
      "content/beam/cb_model/Chat/2-4_512\n",
      "Iteration: 4010; Percent complete: 66.8%; Average loss: 2.4895\n",
      "Iteration: 4020; Percent complete: 67.0%; Average loss: 2.4879\n",
      "Iteration: 4030; Percent complete: 67.2%; Average loss: 2.4595\n",
      "Iteration: 4040; Percent complete: 67.3%; Average loss: 2.4358\n",
      "Iteration: 4050; Percent complete: 67.5%; Average loss: 2.4696\n",
      "Iteration: 4060; Percent complete: 67.7%; Average loss: 2.4249\n",
      "Iteration: 4070; Percent complete: 67.8%; Average loss: 2.4532\n",
      "Iteration: 4080; Percent complete: 68.0%; Average loss: 2.4408\n",
      "Iteration: 4090; Percent complete: 68.2%; Average loss: 2.4777\n",
      "Iteration: 4100; Percent complete: 68.3%; Average loss: 2.4370\n",
      "Iteration: 4110; Percent complete: 68.5%; Average loss: 2.4705\n",
      "Iteration: 4120; Percent complete: 68.7%; Average loss: 2.4344\n",
      "Iteration: 4130; Percent complete: 68.8%; Average loss: 2.4307\n",
      "Iteration: 4140; Percent complete: 69.0%; Average loss: 2.3965\n",
      "Iteration: 4150; Percent complete: 69.2%; Average loss: 2.3868\n",
      "Iteration: 4160; Percent complete: 69.3%; Average loss: 2.4380\n",
      "Iteration: 4170; Percent complete: 69.5%; Average loss: 2.4130\n",
      "Iteration: 4180; Percent complete: 69.7%; Average loss: 2.3792\n",
      "Iteration: 4190; Percent complete: 69.8%; Average loss: 2.4132\n",
      "Iteration: 4200; Percent complete: 70.0%; Average loss: 2.3839\n",
      "Iteration: 4210; Percent complete: 70.2%; Average loss: 2.4033\n",
      "Iteration: 4220; Percent complete: 70.3%; Average loss: 2.3763\n",
      "Iteration: 4230; Percent complete: 70.5%; Average loss: 2.4024\n",
      "Iteration: 4240; Percent complete: 70.7%; Average loss: 2.3893\n",
      "Iteration: 4250; Percent complete: 70.8%; Average loss: 2.3841\n",
      "Iteration: 4260; Percent complete: 71.0%; Average loss: 2.3794\n",
      "Iteration: 4270; Percent complete: 71.2%; Average loss: 2.4027\n",
      "Iteration: 4280; Percent complete: 71.3%; Average loss: 2.3909\n",
      "Iteration: 4290; Percent complete: 71.5%; Average loss: 2.3502\n",
      "Iteration: 4300; Percent complete: 71.7%; Average loss: 2.3497\n",
      "Iteration: 4310; Percent complete: 71.8%; Average loss: 2.3245\n",
      "Iteration: 4320; Percent complete: 72.0%; Average loss: 2.4038\n",
      "Iteration: 4330; Percent complete: 72.2%; Average loss: 2.3587\n",
      "Iteration: 4340; Percent complete: 72.3%; Average loss: 2.3223\n",
      "Iteration: 4350; Percent complete: 72.5%; Average loss: 2.3188\n",
      "Iteration: 4360; Percent complete: 72.7%; Average loss: 2.3212\n",
      "Iteration: 4370; Percent complete: 72.8%; Average loss: 2.3506\n",
      "Iteration: 4380; Percent complete: 73.0%; Average loss: 2.3655\n",
      "Iteration: 4390; Percent complete: 73.2%; Average loss: 2.3559\n",
      "Iteration: 4400; Percent complete: 73.3%; Average loss: 2.3415\n",
      "Iteration: 4410; Percent complete: 73.5%; Average loss: 2.3483\n",
      "Iteration: 4420; Percent complete: 73.7%; Average loss: 2.3497\n",
      "Iteration: 4430; Percent complete: 73.8%; Average loss: 2.3361\n",
      "Iteration: 4440; Percent complete: 74.0%; Average loss: 2.3176\n",
      "Iteration: 4450; Percent complete: 74.2%; Average loss: 2.3321\n",
      "Iteration: 4460; Percent complete: 74.3%; Average loss: 2.3106\n",
      "Iteration: 4470; Percent complete: 74.5%; Average loss: 2.3165\n",
      "Iteration: 4480; Percent complete: 74.7%; Average loss: 2.3257\n",
      "Iteration: 4490; Percent complete: 74.8%; Average loss: 2.2961\n",
      "Iteration: 4500; Percent complete: 75.0%; Average loss: 2.2595\n",
      "Iteration: 4510; Percent complete: 75.2%; Average loss: 2.2890\n",
      "Iteration: 4520; Percent complete: 75.3%; Average loss: 2.2866\n",
      "Iteration: 4530; Percent complete: 75.5%; Average loss: 2.3071\n",
      "Iteration: 4540; Percent complete: 75.7%; Average loss: 2.2972\n",
      "Iteration: 4550; Percent complete: 75.8%; Average loss: 2.2693\n",
      "Iteration: 4560; Percent complete: 76.0%; Average loss: 2.3130\n",
      "Iteration: 4570; Percent complete: 76.2%; Average loss: 2.2435\n",
      "Iteration: 4580; Percent complete: 76.3%; Average loss: 2.2810\n",
      "Iteration: 4590; Percent complete: 76.5%; Average loss: 2.2620\n",
      "Iteration: 4600; Percent complete: 76.7%; Average loss: 2.2369\n",
      "Iteration: 4610; Percent complete: 76.8%; Average loss: 2.2379\n",
      "Iteration: 4620; Percent complete: 77.0%; Average loss: 2.2588\n",
      "Iteration: 4630; Percent complete: 77.2%; Average loss: 2.2387\n",
      "Iteration: 4640; Percent complete: 77.3%; Average loss: 2.2339\n",
      "Iteration: 4650; Percent complete: 77.5%; Average loss: 2.2754\n",
      "Iteration: 4660; Percent complete: 77.7%; Average loss: 2.2347\n",
      "Iteration: 4670; Percent complete: 77.8%; Average loss: 2.1915\n",
      "Iteration: 4680; Percent complete: 78.0%; Average loss: 2.2537\n",
      "Iteration: 4690; Percent complete: 78.2%; Average loss: 2.2309\n",
      "Iteration: 4700; Percent complete: 78.3%; Average loss: 2.2370\n",
      "Iteration: 4710; Percent complete: 78.5%; Average loss: 2.1970\n",
      "Iteration: 4720; Percent complete: 78.7%; Average loss: 2.1904\n",
      "Iteration: 4730; Percent complete: 78.8%; Average loss: 2.1981\n",
      "Iteration: 4740; Percent complete: 79.0%; Average loss: 2.2154\n",
      "Iteration: 4750; Percent complete: 79.2%; Average loss: 2.1888\n",
      "Iteration: 4760; Percent complete: 79.3%; Average loss: 2.2084\n",
      "Iteration: 4770; Percent complete: 79.5%; Average loss: 2.2063\n",
      "Iteration: 4780; Percent complete: 79.7%; Average loss: 2.2061\n",
      "Iteration: 4790; Percent complete: 79.8%; Average loss: 2.1709\n",
      "Iteration: 4800; Percent complete: 80.0%; Average loss: 2.1808\n",
      "Iteration: 4810; Percent complete: 80.2%; Average loss: 2.1374\n",
      "Iteration: 4820; Percent complete: 80.3%; Average loss: 2.1873\n",
      "Iteration: 4830; Percent complete: 80.5%; Average loss: 2.1642\n",
      "Iteration: 4840; Percent complete: 80.7%; Average loss: 2.1716\n",
      "Iteration: 4850; Percent complete: 80.8%; Average loss: 2.1881\n",
      "Iteration: 4860; Percent complete: 81.0%; Average loss: 2.1649\n",
      "Iteration: 4870; Percent complete: 81.2%; Average loss: 2.1818\n",
      "Iteration: 4880; Percent complete: 81.3%; Average loss: 2.1331\n",
      "Iteration: 4890; Percent complete: 81.5%; Average loss: 2.1277\n",
      "Iteration: 4900; Percent complete: 81.7%; Average loss: 2.1840\n",
      "Iteration: 4910; Percent complete: 81.8%; Average loss: 2.1599\n",
      "Iteration: 4920; Percent complete: 82.0%; Average loss: 2.1501\n",
      "Iteration: 4930; Percent complete: 82.2%; Average loss: 2.1486\n",
      "Iteration: 4940; Percent complete: 82.3%; Average loss: 2.1479\n",
      "Iteration: 4950; Percent complete: 82.5%; Average loss: 2.1463\n",
      "Iteration: 4960; Percent complete: 82.7%; Average loss: 2.1256\n",
      "Iteration: 4970; Percent complete: 82.8%; Average loss: 2.1278\n",
      "Iteration: 4980; Percent complete: 83.0%; Average loss: 2.1428\n",
      "Iteration: 4990; Percent complete: 83.2%; Average loss: 2.1338\n",
      "Iteration: 5000; Percent complete: 83.3%; Average loss: 2.1175\n",
      "Iteration: 5010; Percent complete: 83.5%; Average loss: 2.1212\n",
      "Iteration: 5020; Percent complete: 83.7%; Average loss: 2.0816\n",
      "Iteration: 5030; Percent complete: 83.8%; Average loss: 2.1113\n",
      "Iteration: 5040; Percent complete: 84.0%; Average loss: 2.1033\n",
      "Iteration: 5050; Percent complete: 84.2%; Average loss: 2.0958\n",
      "Iteration: 5060; Percent complete: 84.3%; Average loss: 2.0801\n",
      "Iteration: 5070; Percent complete: 84.5%; Average loss: 2.0937\n",
      "Iteration: 5080; Percent complete: 84.7%; Average loss: 2.1087\n",
      "Iteration: 5090; Percent complete: 84.8%; Average loss: 2.1003\n",
      "Iteration: 5100; Percent complete: 85.0%; Average loss: 2.0796\n",
      "Iteration: 5110; Percent complete: 85.2%; Average loss: 2.0911\n",
      "Iteration: 5120; Percent complete: 85.3%; Average loss: 2.0742\n",
      "Iteration: 5130; Percent complete: 85.5%; Average loss: 2.0722\n",
      "Iteration: 5140; Percent complete: 85.7%; Average loss: 2.0762\n",
      "Iteration: 5150; Percent complete: 85.8%; Average loss: 2.0600\n",
      "Iteration: 5160; Percent complete: 86.0%; Average loss: 2.0560\n",
      "Iteration: 5170; Percent complete: 86.2%; Average loss: 2.0704\n",
      "Iteration: 5180; Percent complete: 86.3%; Average loss: 2.0632\n",
      "Iteration: 5190; Percent complete: 86.5%; Average loss: 2.0403\n",
      "Iteration: 5200; Percent complete: 86.7%; Average loss: 2.0439\n",
      "Iteration: 5210; Percent complete: 86.8%; Average loss: 2.0331\n",
      "Iteration: 5220; Percent complete: 87.0%; Average loss: 2.0553\n",
      "Iteration: 5230; Percent complete: 87.2%; Average loss: 2.0255\n",
      "Iteration: 5240; Percent complete: 87.3%; Average loss: 2.0630\n",
      "Iteration: 5250; Percent complete: 87.5%; Average loss: 2.0726\n",
      "Iteration: 5260; Percent complete: 87.7%; Average loss: 2.0322\n",
      "Iteration: 5270; Percent complete: 87.8%; Average loss: 2.0330\n",
      "Iteration: 5280; Percent complete: 88.0%; Average loss: 2.0137\n",
      "Iteration: 5290; Percent complete: 88.2%; Average loss: 2.0442\n",
      "Iteration: 5300; Percent complete: 88.3%; Average loss: 1.9949\n",
      "Iteration: 5310; Percent complete: 88.5%; Average loss: 2.0172\n",
      "Iteration: 5320; Percent complete: 88.7%; Average loss: 2.0300\n",
      "Iteration: 5330; Percent complete: 88.8%; Average loss: 2.0092\n",
      "Iteration: 5340; Percent complete: 89.0%; Average loss: 2.0168\n",
      "Iteration: 5350; Percent complete: 89.2%; Average loss: 1.9817\n",
      "Iteration: 5360; Percent complete: 89.3%; Average loss: 1.9749\n",
      "Iteration: 5370; Percent complete: 89.5%; Average loss: 1.9835\n",
      "Iteration: 5380; Percent complete: 89.7%; Average loss: 1.9706\n",
      "Iteration: 5390; Percent complete: 89.8%; Average loss: 1.9871\n",
      "Iteration: 5400; Percent complete: 90.0%; Average loss: 1.9632\n",
      "Iteration: 5410; Percent complete: 90.2%; Average loss: 2.0019\n",
      "Iteration: 5420; Percent complete: 90.3%; Average loss: 1.9583\n",
      "Iteration: 5430; Percent complete: 90.5%; Average loss: 1.9505\n",
      "Iteration: 5440; Percent complete: 90.7%; Average loss: 1.9811\n",
      "Iteration: 5450; Percent complete: 90.8%; Average loss: 1.9631\n",
      "Iteration: 5460; Percent complete: 91.0%; Average loss: 1.9343\n",
      "Iteration: 5470; Percent complete: 91.2%; Average loss: 1.9415\n",
      "Iteration: 5480; Percent complete: 91.3%; Average loss: 1.9443\n",
      "Iteration: 5490; Percent complete: 91.5%; Average loss: 1.9120\n",
      "Iteration: 5500; Percent complete: 91.7%; Average loss: 1.9642\n",
      "Iteration: 5510; Percent complete: 91.8%; Average loss: 1.9426\n",
      "Iteration: 5520; Percent complete: 92.0%; Average loss: 1.8965\n",
      "Iteration: 5530; Percent complete: 92.2%; Average loss: 1.9313\n",
      "Iteration: 5540; Percent complete: 92.3%; Average loss: 1.9349\n",
      "Iteration: 5550; Percent complete: 92.5%; Average loss: 1.9522\n",
      "Iteration: 5560; Percent complete: 92.7%; Average loss: 1.9360\n",
      "Iteration: 5570; Percent complete: 92.8%; Average loss: 1.9264\n",
      "Iteration: 5580; Percent complete: 93.0%; Average loss: 1.8977\n",
      "Iteration: 5590; Percent complete: 93.2%; Average loss: 1.9215\n",
      "Iteration: 5600; Percent complete: 93.3%; Average loss: 1.9237\n",
      "Iteration: 5610; Percent complete: 93.5%; Average loss: 1.8890\n",
      "Iteration: 5620; Percent complete: 93.7%; Average loss: 1.9048\n",
      "Iteration: 5630; Percent complete: 93.8%; Average loss: 1.9068\n",
      "Iteration: 5640; Percent complete: 94.0%; Average loss: 1.9045\n",
      "Iteration: 5650; Percent complete: 94.2%; Average loss: 1.8486\n",
      "Iteration: 5660; Percent complete: 94.3%; Average loss: 1.8958\n",
      "Iteration: 5670; Percent complete: 94.5%; Average loss: 1.8600\n",
      "Iteration: 5680; Percent complete: 94.7%; Average loss: 1.8974\n",
      "Iteration: 5690; Percent complete: 94.8%; Average loss: 1.8275\n",
      "Iteration: 5700; Percent complete: 95.0%; Average loss: 1.8774\n",
      "Iteration: 5710; Percent complete: 95.2%; Average loss: 1.8642\n",
      "Iteration: 5720; Percent complete: 95.3%; Average loss: 1.9147\n",
      "Iteration: 5730; Percent complete: 95.5%; Average loss: 1.8571\n",
      "Iteration: 5740; Percent complete: 95.7%; Average loss: 1.8788\n",
      "Iteration: 5750; Percent complete: 95.8%; Average loss: 1.8228\n",
      "Iteration: 5760; Percent complete: 96.0%; Average loss: 1.8714\n",
      "Iteration: 5770; Percent complete: 96.2%; Average loss: 1.8544\n",
      "Iteration: 5780; Percent complete: 96.3%; Average loss: 1.8524\n",
      "Iteration: 5790; Percent complete: 96.5%; Average loss: 1.8717\n",
      "Iteration: 5800; Percent complete: 96.7%; Average loss: 1.8500\n",
      "Iteration: 5810; Percent complete: 96.8%; Average loss: 1.8666\n",
      "Iteration: 5820; Percent complete: 97.0%; Average loss: 1.8433\n",
      "Iteration: 5830; Percent complete: 97.2%; Average loss: 1.8255\n",
      "Iteration: 5840; Percent complete: 97.3%; Average loss: 1.8458\n",
      "Iteration: 5850; Percent complete: 97.5%; Average loss: 1.8075\n",
      "Iteration: 5860; Percent complete: 97.7%; Average loss: 1.8282\n",
      "Iteration: 5870; Percent complete: 97.8%; Average loss: 1.8654\n",
      "Iteration: 5880; Percent complete: 98.0%; Average loss: 1.8225\n",
      "Iteration: 5890; Percent complete: 98.2%; Average loss: 1.7983\n",
      "Iteration: 5900; Percent complete: 98.3%; Average loss: 1.8047\n",
      "Iteration: 5910; Percent complete: 98.5%; Average loss: 1.8012\n",
      "Iteration: 5920; Percent complete: 98.7%; Average loss: 1.8122\n",
      "Iteration: 5930; Percent complete: 98.8%; Average loss: 1.8057\n",
      "Iteration: 5940; Percent complete: 99.0%; Average loss: 1.7791\n",
      "Iteration: 5950; Percent complete: 99.2%; Average loss: 1.8054\n",
      "Iteration: 5960; Percent complete: 99.3%; Average loss: 1.7852\n",
      "Iteration: 5970; Percent complete: 99.5%; Average loss: 1.7875\n",
      "Iteration: 5980; Percent complete: 99.7%; Average loss: 1.8060\n",
      "Iteration: 5990; Percent complete: 99.8%; Average loss: 1.8011\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 1.7820\n",
      "content/beam/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "from tqdm.contrib.discord import tqdm, trange\n",
    "save_dir = 'content/beam'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 10\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 362.5625 248.518125\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 362.5625 248.518125 \nL 362.5625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 224.64 \nL 355.3625 224.64 \nL 355.3625 7.2 \nL 20.5625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m1146cde8a2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#m1146cde8a2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(32.599432 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.592641\" xlink:href=\"#m1146cde8a2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(77.048891 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"137.4046\" xlink:href=\"#m1146cde8a2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(127.86085 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.21656\" xlink:href=\"#m1146cde8a2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(178.67281 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.028519\" xlink:href=\"#m1146cde8a2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(229.484769 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.840478\" xlink:href=\"#m1146cde8a2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(280.296728 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"340.652438\" xlink:href=\"#m1146cde8a2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(331.108688 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb57cbeee76\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mb57cbeee76\" y=\"207.999025\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 211.798243)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mb57cbeee76\" y=\"177.40911\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 181.208329)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mb57cbeee76\" y=\"146.819195\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 150.618414)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mb57cbeee76\" y=\"116.229281\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(7.2 120.0285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mb57cbeee76\" y=\"85.639366\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 6 -->\n      <g transform=\"translate(7.2 89.438585)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mb57cbeee76\" y=\"55.049452\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 58.84867)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mb57cbeee76\" y=\"24.459537\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(7.2 28.258756)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p84f66bc301)\" d=\"M 35.780682 17.083636 \nL 36.288801 91.537578 \nL 36.796921 114.312765 \nL 37.305041 115.790565 \nL 38.32128 120.896465 \nL 38.829399 122.244815 \nL 39.337519 122.921016 \nL 39.845639 122.667494 \nL 40.353758 124.149707 \nL 40.861878 123.989663 \nL 41.369997 124.821382 \nL 41.878117 124.552041 \nL 42.386237 125.251376 \nL 42.894356 126.510477 \nL 43.402476 125.547582 \nL 43.910595 126.846857 \nL 44.418715 126.639023 \nL 44.926834 127.473632 \nL 45.434954 128.613168 \nL 45.943074 132.818742 \nL 46.451193 132.82178 \nL 46.959313 133.943985 \nL 47.467432 132.746562 \nL 47.975552 135.101392 \nL 48.483672 135.272844 \nL 48.991791 134.751976 \nL 49.499911 136.493582 \nL 50.00803 136.211842 \nL 51.02427 134.807148 \nL 52.040509 136.627981 \nL 52.548628 136.252993 \nL 53.056748 137.601159 \nL 53.564868 136.941533 \nL 54.072987 137.219947 \nL 54.581107 138.292679 \nL 55.597346 138.35169 \nL 56.105466 139.895131 \nL 56.613585 139.074869 \nL 57.629824 141.432262 \nL 58.137944 140.205476 \nL 59.154183 142.54845 \nL 59.662303 141.514526 \nL 60.170422 141.313524 \nL 60.678542 142.136051 \nL 61.186661 144.31906 \nL 61.694781 141.986335 \nL 62.202901 144.96115 \nL 62.71102 143.753643 \nL 63.21914 142.934044 \nL 63.727259 144.449314 \nL 64.235379 144.376871 \nL 64.743499 142.777168 \nL 65.251618 146.549167 \nL 65.759738 145.143283 \nL 66.267857 145.877944 \nL 66.775977 145.069333 \nL 67.284097 146.533785 \nL 68.300336 146.447844 \nL 68.808455 146.443879 \nL 69.316575 147.083773 \nL 69.824695 146.43672 \nL 70.332814 147.75511 \nL 70.840934 147.745123 \nL 71.349053 148.767185 \nL 71.857173 147.405665 \nL 72.365293 148.208388 \nL 72.873412 147.86343 \nL 73.381532 148.149455 \nL 73.889651 148.655806 \nL 74.397771 147.412559 \nL 74.90589 149.475527 \nL 75.41401 148.766016 \nL 75.92213 149.854281 \nL 76.430249 149.907318 \nL 76.938369 149.111819 \nL 77.446488 150.221224 \nL 77.954608 149.445814 \nL 78.462728 149.927137 \nL 78.970847 149.226548 \nL 79.478967 149.633543 \nL 79.987086 150.765596 \nL 80.495206 150.039785 \nL 81.003326 151.157428 \nL 81.511445 150.087509 \nL 82.019565 151.016822 \nL 82.527684 150.729144 \nL 83.035804 151.411653 \nL 83.543924 151.231927 \nL 84.052043 152.677168 \nL 84.560163 150.680064 \nL 85.068282 152.668872 \nL 85.576402 152.276638 \nL 86.084522 152.1142 \nL 86.592641 153.894994 \nL 87.100761 154.169402 \nL 87.60888 153.466229 \nL 88.117 153.132929 \nL 88.62512 153.227971 \nL 89.133239 153.538953 \nL 89.641359 154.478883 \nL 90.149478 155.706231 \nL 90.657598 153.995442 \nL 91.165717 154.757707 \nL 91.673837 153.526499 \nL 92.181957 154.788087 \nL 92.690076 154.693474 \nL 93.198196 155.730808 \nL 93.706315 155.892196 \nL 94.214435 155.4185 \nL 94.722555 156.37317 \nL 95.230674 156.401769 \nL 95.738794 156.620829 \nL 96.246913 157.025748 \nL 96.755033 157.025391 \nL 97.263153 156.357805 \nL 97.771272 156.607413 \nL 98.279392 157.246857 \nL 98.787511 158.366673 \nL 99.295631 157.476452 \nL 99.803751 158.08642 \nL 100.31187 157.588405 \nL 100.81999 157.625763 \nL 101.328109 159.285313 \nL 101.836229 159.297845 \nL 102.344349 158.374449 \nL 102.852468 158.500385 \nL 103.868707 160.883876 \nL 104.376827 159.832289 \nL 104.884947 159.173298 \nL 105.393066 160.103654 \nL 105.901186 159.961685 \nL 106.409305 160.039098 \nL 106.917425 159.771837 \nL 107.425544 159.892989 \nL 107.933664 160.508537 \nL 109.458023 161.685718 \nL 109.966142 161.377116 \nL 110.474262 162.04354 \nL 110.982382 160.89855 \nL 111.490501 163.184737 \nL 111.998621 162.118464 \nL 112.50674 162.367326 \nL 113.01486 162.483579 \nL 113.52298 163.352492 \nL 114.031099 161.066577 \nL 114.539219 162.700203 \nL 115.047338 162.175314 \nL 115.555458 163.747044 \nL 116.063578 162.639047 \nL 116.571697 163.040467 \nL 117.079817 163.214352 \nL 117.587936 162.580819 \nL 118.096056 164.677895 \nL 118.604176 165.174236 \nL 119.112295 163.813368 \nL 119.620415 164.580376 \nL 120.128534 164.078123 \nL 120.636654 165.092985 \nL 121.652893 164.299114 \nL 122.161013 164.19531 \nL 122.669132 165.959207 \nL 123.177252 164.424528 \nL 123.685371 165.959149 \nL 124.701611 165.797283 \nL 125.20973 166.609641 \nL 125.71785 166.07525 \nL 126.225969 166.78048 \nL 126.734089 166.008243 \nL 127.242209 167.543918 \nL 127.750328 165.493485 \nL 128.258448 166.441029 \nL 129.274687 167.387617 \nL 129.782807 167.294435 \nL 130.290926 166.867427 \nL 130.799046 167.934929 \nL 131.307165 167.108003 \nL 131.815285 167.697708 \nL 132.323405 167.425727 \nL 132.831524 166.617854 \nL 133.339644 168.76802 \nL 133.847763 167.637837 \nL 134.355883 167.814027 \nL 134.864003 167.14538 \nL 135.372122 170.057002 \nL 136.388361 168.466749 \nL 136.896481 168.118435 \nL 137.4046 169.037731 \nL 137.91272 167.991852 \nL 138.42084 169.596832 \nL 138.928959 168.800145 \nL 139.437079 169.311666 \nL 139.945198 169.006786 \nL 140.453318 169.927999 \nL 140.961438 169.337096 \nL 141.469557 169.569561 \nL 141.977677 171.238282 \nL 142.485796 170.838536 \nL 142.993916 170.91385 \nL 143.502036 169.729297 \nL 144.010155 171.791369 \nL 144.518275 171.259313 \nL 145.026394 169.078619 \nL 146.042634 171.249805 \nL 146.550753 171.744069 \nL 147.058873 171.185397 \nL 147.566992 172.300161 \nL 148.075112 171.661512 \nL 148.583232 171.948477 \nL 149.091351 172.397032 \nL 149.599471 172.435602 \nL 150.10759 171.723307 \nL 150.61571 172.919383 \nL 151.123829 172.6642 \nL 151.631949 171.469436 \nL 152.140069 173.324637 \nL 152.648188 172.622209 \nL 153.156308 172.721808 \nL 153.664427 172.427331 \nL 154.680667 173.080293 \nL 155.188786 173.00766 \nL 155.696906 173.233407 \nL 156.205025 173.589116 \nL 156.713145 174.456579 \nL 157.221265 173.399889 \nL 157.729384 172.862554 \nL 158.237504 174.120962 \nL 158.745623 173.814941 \nL 159.253743 174.717419 \nL 159.761863 174.141387 \nL 160.269982 175.022845 \nL 160.778102 175.450236 \nL 161.286221 175.174352 \nL 161.794341 174.739256 \nL 162.302461 175.701285 \nL 162.81058 175.727736 \nL 163.3187 175.991139 \nL 163.826819 174.522323 \nL 164.334939 175.53874 \nL 164.843059 175.371896 \nL 165.351178 176.831727 \nL 165.859298 176.32444 \nL 166.367417 176.023804 \nL 166.875537 176.889722 \nL 167.383656 175.217713 \nL 167.891776 177.632236 \nL 168.399896 176.475457 \nL 168.908015 176.984621 \nL 169.416135 176.159314 \nL 169.924254 176.905452 \nL 170.432374 177.238083 \nL 170.940494 178.343758 \nL 171.448613 177.253513 \nL 172.464852 178.481624 \nL 172.972972 177.137549 \nL 173.481092 177.548369 \nL 173.989211 178.911243 \nL 174.497331 178.541216 \nL 175.51357 179.052639 \nL 176.02169 178.295153 \nL 176.529809 178.328122 \nL 177.037929 179.351172 \nL 177.546048 178.948005 \nL 178.054168 178.92447 \nL 178.562288 179.770819 \nL 179.070407 179.357677 \nL 179.578527 180.005315 \nL 180.086646 180.136321 \nL 180.594766 180.1349 \nL 181.102885 179.342071 \nL 181.611005 180.482093 \nL 182.119125 180.70873 \nL 182.627244 178.631629 \nL 183.135364 181.516709 \nL 183.643483 180.21416 \nL 184.659723 181.491686 \nL 185.167842 180.820092 \nL 185.675962 180.927575 \nL 186.184081 180.433391 \nL 186.692201 179.637578 \nL 187.200321 181.983197 \nL 187.70844 181.390587 \nL 188.21656 181.153264 \nL 188.724679 182.346893 \nL 189.232799 181.34331 \nL 189.740919 181.561504 \nL 190.757158 181.654769 \nL 191.265277 182.308913 \nL 191.773397 182.761393 \nL 192.281517 181.328293 \nL 192.789636 182.860088 \nL 193.297756 183.42729 \nL 194.313995 183.852653 \nL 195.330234 183.776191 \nL 195.838354 182.85718 \nL 196.346473 184.191278 \nL 196.854593 183.868339 \nL 197.362712 184.06924 \nL 197.870832 182.364195 \nL 198.378952 184.436223 \nL 198.887071 184.293683 \nL 199.395191 182.783445 \nL 199.90331 182.975972 \nL 200.41143 184.360493 \nL 200.91955 184.396211 \nL 201.427669 185.576242 \nL 201.935789 184.388665 \nL 202.952028 185.609053 \nL 203.460148 184.688063 \nL 203.968267 184.958721 \nL 204.476387 185.076038 \nL 204.984506 185.529987 \nL 205.492626 185.667987 \nL 206.000746 186.476893 \nL 206.508865 185.579439 \nL 207.016985 186.135666 \nL 207.525104 186.453042 \nL 208.033224 185.776274 \nL 208.541344 185.7767 \nL 209.049463 187.882578 \nL 209.557583 187.155952 \nL 210.065702 186.222689 \nL 210.573822 186.908057 \nL 211.590061 188.54457 \nL 212.098181 187.332935 \nL 212.6063 186.654697 \nL 213.11442 186.800568 \nL 213.622539 187.822309 \nL 214.130659 188.23774 \nL 214.638779 187.64284 \nL 215.655018 186.953628 \nL 217.687496 189.417402 \nL 218.195616 188.972432 \nL 219.211855 189.497758 \nL 219.719975 188.600968 \nL 220.228094 188.434582 \nL 221.244333 190.130705 \nL 221.752453 189.255164 \nL 222.260573 189.27913 \nL 222.768692 190.390819 \nL 223.276812 189.170775 \nL 223.784931 189.699737 \nL 224.293051 191.338353 \nL 224.801171 190.835791 \nL 225.81741 190.641295 \nL 226.325529 189.884056 \nL 226.833649 190.477617 \nL 227.341768 190.802197 \nL 227.849888 190.213776 \nL 228.358008 190.137318 \nL 228.866127 190.554231 \nL 229.374247 191.538818 \nL 229.882366 191.328605 \nL 230.390486 191.363834 \nL 230.898606 191.988367 \nL 231.406725 191.495574 \nL 231.914845 191.450085 \nL 232.422964 193.332041 \nL 232.931084 193.055976 \nL 233.439204 192.415356 \nL 233.947323 192.905077 \nL 234.455443 192.369012 \nL 234.963562 193.795888 \nL 235.471682 192.535222 \nL 235.979802 192.074227 \nL 236.487921 192.323222 \nL 236.996041 192.786975 \nL 237.50416 194.120417 \nL 238.01228 193.677402 \nL 238.5204 193.798702 \nL 239.028519 193.025656 \nL 239.536639 193.07549 \nL 240.552878 194.666765 \nL 241.060997 193.633017 \nL 241.569117 195.001927 \nL 242.077237 194.137141 \nL 242.585356 194.515361 \nL 243.093476 193.387512 \nL 243.601595 194.630596 \nL 244.109715 193.606245 \nL 244.617835 194.709703 \nL 245.125954 194.822844 \nL 245.634074 195.870862 \nL 246.142193 196.166897 \nL 246.650313 194.599697 \nL 247.158433 195.36533 \nL 247.666552 196.400017 \nL 248.174672 195.359374 \nL 248.682791 196.254758 \nL 249.190911 195.661856 \nL 249.699031 196.487447 \nL 250.20715 195.689212 \nL 250.71527 196.091868 \nL 251.731509 196.394274 \nL 252.239629 195.680344 \nL 252.747748 196.041667 \nL 253.255868 197.286573 \nL 253.763987 197.302643 \nL 254.272107 198.071729 \nL 254.780227 195.647311 \nL 255.796466 198.140056 \nL 256.304585 198.248239 \nL 256.812705 198.173227 \nL 257.320824 197.27364 \nL 257.828944 196.818387 \nL 258.337064 197.113497 \nL 258.845183 197.551508 \nL 259.353303 197.343226 \nL 259.861422 197.30062 \nL 260.369542 197.717564 \nL 260.877662 198.283885 \nL 261.385781 197.840879 \nL 261.893901 198.49732 \nL 262.91014 198.037314 \nL 263.926379 200.062298 \nL 264.434499 199.159968 \nL 264.942618 199.231773 \nL 265.450738 198.604433 \nL 265.958858 198.908237 \nL 266.466977 199.760844 \nL 266.975097 198.425855 \nL 267.483216 200.54898 \nL 267.991336 199.403606 \nL 268.499456 199.982979 \nL 269.007575 200.751787 \nL 269.515695 200.721271 \nL 270.023814 200.082592 \nL 270.531934 200.698359 \nL 271.040053 200.842988 \nL 271.548173 199.575111 \nL 272.564412 202.139831 \nL 273.072532 200.237834 \nL 273.580651 200.936956 \nL 274.088771 200.750343 \nL 274.596891 201.971805 \nL 275.10501 202.173717 \nL 275.61313 201.938413 \nL 276.121249 201.410676 \nL 276.629369 202.223293 \nL 277.137489 201.623242 \nL 278.153728 201.693323 \nL 278.661847 202.772064 \nL 279.169967 202.467553 \nL 279.678087 203.795494 \nL 280.186206 202.270134 \nL 280.694326 202.976488 \nL 281.202445 202.749414 \nL 281.710565 202.24443 \nL 282.218685 202.955178 \nL 282.726804 202.438985 \nL 283.234924 203.927774 \nL 283.743043 204.091907 \nL 284.251163 202.369734 \nL 284.759283 203.108287 \nL 285.267402 203.40621 \nL 286.791761 203.523442 \nL 287.29988 204.1577 \nL 287.808 204.09034 \nL 288.31612 203.630315 \nL 288.824239 203.906592 \nL 289.332359 204.406236 \nL 289.840478 204.29248 \nL 290.348598 205.503745 \nL 290.856718 204.595674 \nL 291.872957 205.068568 \nL 292.381076 205.549638 \nL 293.397316 204.67412 \nL 293.905435 204.930661 \nL 294.413555 205.562718 \nL 294.921674 205.211717 \nL 295.429794 205.729396 \nL 295.937914 205.789887 \nL 296.446033 205.667981 \nL 296.954153 206.163926 \nL 297.462272 206.286563 \nL 297.970392 205.84696 \nL 298.478512 206.065695 \nL 298.986631 206.766297 \nL 299.494751 206.655809 \nL 300.00287 206.985479 \nL 300.51099 206.308871 \nL 301.01911 207.219311 \nL 301.527229 206.071863 \nL 302.035349 205.7793 \nL 302.543468 207.013577 \nL 303.051588 206.990782 \nL 303.559707 207.580751 \nL 304.067827 206.648005 \nL 304.575947 208.156378 \nL 305.084066 207.473708 \nL 305.592186 207.081568 \nL 306.100305 207.718749 \nL 306.608425 207.485384 \nL 307.116545 208.557789 \nL 307.624664 208.768156 \nL 308.132784 208.505063 \nL 308.640903 208.897629 \nL 309.149023 208.392313 \nL 309.657143 209.1252 \nL 310.165262 207.942183 \nL 310.673382 209.273352 \nL 311.181501 209.514294 \nL 311.689621 208.575671 \nL 312.197741 209.128677 \nL 312.70586 210.008521 \nL 313.21398 209.78869 \nL 313.722099 209.703576 \nL 314.230219 210.690609 \nL 314.738339 209.092659 \nL 315.246458 209.754821 \nL 315.754578 211.164775 \nL 316.262697 210.1019 \nL 316.770817 209.990168 \nL 317.278936 209.461205 \nL 317.787056 209.957786 \nL 318.295176 210.249811 \nL 318.803295 211.12851 \nL 319.311415 210.399896 \nL 319.819534 210.332996 \nL 320.327654 211.395259 \nL 320.835774 210.909994 \nL 321.343893 210.85144 \nL 321.852013 210.919491 \nL 322.360132 212.63066 \nL 322.868252 211.186378 \nL 323.376372 212.28269 \nL 323.884491 211.138726 \nL 324.392611 213.276048 \nL 324.90073 211.750692 \nL 325.40885 212.154311 \nL 325.91697 210.608503 \nL 326.425089 212.369889 \nL 326.933209 211.707559 \nL 327.441328 213.419748 \nL 327.949448 211.93331 \nL 328.457568 212.453956 \nL 328.965687 212.513665 \nL 329.473807 211.922964 \nL 329.981926 212.587174 \nL 330.490046 212.078644 \nL 331.506285 213.335961 \nL 332.014405 212.714613 \nL 332.522524 213.887025 \nL 333.030644 213.255693 \nL 333.538763 212.117535 \nL 334.046883 213.429523 \nL 334.555003 214.170254 \nL 335.063122 213.973 \nL 335.571242 214.079346 \nL 336.079361 213.745097 \nL 336.587481 213.94374 \nL 337.095601 214.756364 \nL 337.60372 213.951096 \nL 338.11184 214.569353 \nL 338.619959 214.500748 \nL 339.128079 213.932105 \nL 339.636199 214.083221 \nL 340.144318 214.668613 \nL 340.144318 214.668613 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 224.64 \nL 20.5625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 355.3625 224.64 \nL 355.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 224.64 \nL 355.3625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 7.2 \nL 355.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p84f66bc301\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Bleu score Calculation for Beam search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.13406400920712788 0.04739878501170794\n",
      "1000 0.15674150177584226 0.07069958145080933\n",
      "2000 0.14601108728119894 0.06506785101638089\n",
      "3000 0.14666470225263117 0.06565489601654281\n",
      "4000 0.14810825443924747 0.06559820531880635\n",
      "5000 0.14754701043529136 0.06574311412989854\n",
      "6000 0.14687792357817048 0.06503098534501695\n",
      "7000 0.15310102450351762 0.07107164419517663\n",
      "8000 0.15160559356415987 0.07011479924099687\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.15082862204169686\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.06961184299487612\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "############################################################################\n",
    "# Difference between greedy search and beam search is here\n",
    "\n",
    "# greedy search\n",
    "# searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# beam search\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "############################################################################\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "  \n",
    "  input_sentence = testpairs[i][0]\n",
    "  \n",
    "  reference = testpairs[i][1:]\n",
    "  templist = []\n",
    "  for k in range(len(reference)):\n",
    "    if(reference[k]!=''):\n",
    "      temp = reference[k].split(' ')\n",
    "      templist.append(temp)\n",
    "  \n",
    "  \n",
    "  input_sentence = normalizeString(input_sentence)\n",
    "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "  chencherry = SmoothingFunction()\n",
    "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "  gram1_bleu_score.append(score1)\n",
    "  gram2_bleu_score.append(score2)\n",
    "  if i%1000 == 0:\n",
    "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bot: it s me .\n",
      "Bot: just fine i m fine .\n",
      "Bot: of course .\n",
      "Bot: thank you .\n",
      "Bot: i love you .\n",
      "Bot: i don t think so .\n",
      "Bot: i don t want to talk about it .\n",
      "Bot: i m sorry .\n",
      "Bot: i don t want to know\n",
      "Bot: hello what are you doing here ?\n",
      "Bot: what ?\n",
      "Bot: yeah i think so .\n",
      "Bot: i want to go back to\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-72fb9220f3db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeamSearchDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-ec18c2cf8348>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}